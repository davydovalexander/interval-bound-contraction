{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "fb8c1b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "[0.01235143 0.01243659 0.02712459]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x140d2a7e0>]"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cvxpy as cp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def drift(x):\n",
    "    return np.array([-x[0] + x[2], \n",
    "    x[0]**2 - x[1] - 2*x[0]*x[2] + x[2], \n",
    "    -x[1]])\n",
    "\n",
    "def NCM(x):\n",
    "    return np.eye(3)\n",
    "\n",
    "def smooth_relu(x):\n",
    "    return np.log(1 + np.exp(x))\n",
    "\n",
    "def multiply_diagonal(W, L_diag, U_diag, side='right'):\n",
    "    \"\"\"\n",
    "    Multiply matrix W with diagonal interval matrix [L_diag, U_diag].\n",
    "    \n",
    "    If side = 'right', returns bounds for W * J\n",
    "    If side = 'left',  returns bounds for J * W\n",
    "    \"\"\"\n",
    "    L_diag = np.asarray(L_diag).reshape(-1)\n",
    "    U_diag = np.asarray(U_diag).reshape(-1)\n",
    "\n",
    "    if side == 'right':\n",
    "        prod1 = W * L_diag  # broadcasting\n",
    "        prod2 = W * U_diag\n",
    "    elif side == 'left':\n",
    "        prod1 = (L_diag[:, None]) * W\n",
    "        prod2 = (U_diag[:, None]) * W\n",
    "    else:\n",
    "        raise ValueError(\"side must be 'left' or 'right'\")\n",
    "\n",
    "    lower = np.minimum(prod1, prod2)\n",
    "    upper = np.maximum(prod1, prod2)\n",
    "    return lower, upper\n",
    "\n",
    "def multiply_known_W(W, lower, upper, side='right'):\n",
    "    if side == 'right':\n",
    "        lower_new = lower @ W\n",
    "        upper_new = upper @ W\n",
    "    elif side == 'left':\n",
    "        lower_new = W @ lower\n",
    "        upper_new = W @ upper\n",
    "    else:\n",
    "        raise ValueError(\"side must be 'left' or 'right'\")\n",
    "    lower, upper = np.minimum(lower_new, upper_new), np.maximum(lower_new,upper_new)\n",
    "    return lower- upper\n",
    "\n",
    "B = np.array([0, 0, 1])\n",
    "\n",
    "T = 1000\n",
    "dt = 0.01\n",
    "\n",
    "xs = np.zeros((3, T))\n",
    "x = xs[:,0] + 2.2\n",
    "print(x.size)\n",
    "for i in range(T):\n",
    "    x += dt * drift(x)\n",
    "    xs[:,i] = x\n",
    "\n",
    "print(xs[:,-1])\n",
    "\n",
    "plt.plot(xs[2,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5fb6256",
   "metadata": {},
   "outputs": [],
   "source": [
    "xlow = np.array([-0.5, -0.5, -0.5])\n",
    "xhigh = np.array([0.5, 0.5, 0.5])\n",
    "\n",
    "W1 = np.random.randn(3, 16)\n",
    "W2 = np.random.randn(16)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab574e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = np.array([[1.0, 2.0], [-1.0, 0.5]])\n",
    "W2 = np.array([[0.5, -1.0], [2.0,  1.0]])\n",
    "W3 = np.array([[1.0, 0.0], [0.0, 1.0]])\n",
    "\n",
    "L1 = np.array([0.5, -1.0])\n",
    "U1 = np.array([1.5,  0.0])\n",
    "\n",
    "L2 = np.array([-0.5, 0.2])\n",
    "U2 = np.array([1.0,  1.0])\n",
    "\n",
    "'''\n",
    "# TODOs\n",
    "1. Compute intermediate interval bounds on the z_k\n",
    "2. Compute the bounds on the J_i, L_i <= J_i <= U_i\n",
    "3. Compute the bounds then on the product defining Du(x)\n",
    "'''\n",
    "\n",
    "def linear_interval(W, l, u):\n",
    "    W_plus = np.maximum(W, 0)\n",
    "    W_minus = W - W_plus\n",
    "    \n",
    "    lower = W_plus @ l + W_minus @ u\n",
    "    upper = W_plus @ u + W_minus @ l\n",
    "    return lower, upper\n",
    "\n",
    "def get_hidden_preactivation_bounds(x_lower, x_over, W1, W2):\n",
    "    bounds = []\n",
    "    l1, u1 = linear_interval(W1, x_lower, x_over)\n",
    "\n",
    "    l_inter, u_inter = smooth_relu(l1), smooth_relu(u1)\n",
    "    l2, u2 = linear_interval(W2, l1, u1)\n",
    "    return l1, u1\n",
    "\n",
    "def get_diagonal_bounds(l, u):\n",
    "    return smooth_deriv(l), smooth_deriv(u)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "af125e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class NN_IBP(nn.Module):\n",
    "    def __init__(self, input_dim=3, hidden_dims=[16, 16], output_dim=1, activation='softplus', trainable_NCM=False):\n",
    "        super().__init__()\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        self.activations = nn.ModuleList()\n",
    "        if trainable_NCM:\n",
    "            self.P = nn.Linear(input_dim, input_dim, bias = False)\n",
    "            # self.P = torch.randn((3,3), requires_grad=True)\n",
    "        else:\n",
    "            self.P = nn.Linear(input_dim, input_dim, bias = False)\n",
    "            self.P.weight.requires_grad = False\n",
    "\n",
    "        dims = [input_dim] + hidden_dims\n",
    "        for i in range(len(hidden_dims)):\n",
    "            self.hidden_layers.append(nn.Linear(dims[i], dims[i+1]))\n",
    "            if activation == 'softplus':\n",
    "                self.activations.append(nn.Softplus())\n",
    "            elif activation == 'relu':\n",
    "                self.activations.append(nn.ReLU())\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported activation: {activation}\")\n",
    "\n",
    "        self.output_layer = nn.Linear(hidden_dims[-1], output_dim)\n",
    "\n",
    "    def constant_NCM(self):\n",
    "        return self.P.weight @ torch.transpose(self.P.weight, 0, 1) + torch.eye(self.P.weight.shape[-1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer, act in zip(self.hidden_layers, self.activations):\n",
    "            x = act(layer(x))\n",
    "        return self.output_layer(x)\n",
    "\n",
    "    def get_hidden_pre_activation_bounds(self, x_lower, x_upper):\n",
    "        \"\"\"\n",
    "        Compute pre-activation interval bounds for each hidden layer.\n",
    "\n",
    "        Args:\n",
    "            x_lower: Tensor of shape (B, input_dim)\n",
    "            x_upper: Tensor of shape (B, input_dim)\n",
    "\n",
    "        Returns:\n",
    "            List of tuples (l_i, u_i): pre-activation bounds at each hidden layer\n",
    "        \"\"\"\n",
    "        def linear_interval(layer: nn.Linear, l, u):\n",
    "            W = layer.weight\n",
    "            b = layer.bias\n",
    "            W_pos = torch.clamp(W, min=0)\n",
    "            W_neg = torch.clamp(W, max=0)\n",
    "\n",
    "            lower = torch.matmul(l, W_pos.T) + torch.matmul(u, W_neg.T) + b\n",
    "            upper = torch.matmul(u, W_pos.T) + torch.matmul(l, W_neg.T) + b\n",
    "            return lower, upper\n",
    "\n",
    "        def activation_interval(act, l, u):\n",
    "            return act(l), act(u)\n",
    "\n",
    "        bounds = []\n",
    "\n",
    "        l, u = x_lower, x_upper\n",
    "        for layer, act in zip(self.hidden_layers, self.activations):\n",
    "            l_pre, u_pre = linear_interval(layer, l, u)\n",
    "            bounds.append((l_pre, u_pre))\n",
    "            l, u = activation_interval(act, l_pre, u_pre)\n",
    "\n",
    "        return bounds\n",
    "    \n",
    "    def activation_derivative(self, x):\n",
    "        # return 1/(1 + torch.exp(-x))\n",
    "        alpha = 0.3\n",
    "        return alpha + (1 - alpha) * torch.sigmoid(x)\n",
    "\n",
    "    def get_diagonal_bounds_from_intermediate(self, pre_act_bounds):\n",
    "        diagonal_bounds = []\n",
    "        for i in range(len(pre_act_bounds)):\n",
    "            lower_diag_bound = self.activation_derivative(pre_act_bounds[i][0])\n",
    "            upper_diag_bound = self.activation_derivative(pre_act_bounds[i][1])\n",
    "            diagonal_bounds.append((lower_diag_bound, upper_diag_bound))\n",
    "        return diagonal_bounds\n",
    "\n",
    "    def get_diag_bounds(self, l, u):\n",
    "        pre_act_bounds = self.get_hidden_pre_activation_bounds(l, u)\n",
    "        return self.get_diagonal_bounds_from_intermediate(pre_act_bounds)\n",
    "    \n",
    "    def left_multiply_by_diag(self, J_lower, J_upper, P_lower, P_upper):\n",
    "        \"\"\"\n",
    "        Batched version: Compute bounds on J @ P, where J is diagonal\n",
    "        with diag entries in [J_lower, J_upper], and P is an interval matrix.\n",
    "\n",
    "        Args:\n",
    "            J_lower, J_upper: (b, n)\n",
    "            P_lower, P_upper: (b, n, m)\n",
    "\n",
    "        Returns:\n",
    "            (lower, upper): (b, n, m)\n",
    "        \"\"\"\n",
    "        # Expand diagonal bounds to match P's shape for broadcasting\n",
    "        J_lower_exp = J_lower.unsqueeze(-1)  # (b, n, 1)\n",
    "        J_upper_exp = J_upper.unsqueeze(-1)  # (b, n, 1)\n",
    "\n",
    "        # Four possible products\n",
    "        ll = J_lower_exp * P_lower\n",
    "        lu = J_lower_exp * P_upper\n",
    "        ul = J_upper_exp * P_lower\n",
    "        uu = J_upper_exp * P_upper\n",
    "\n",
    "        # Elementwise min/max over the four cases\n",
    "        lower = torch.minimum(torch.minimum(ll, lu), torch.minimum(ul, uu))\n",
    "        upper = torch.maximum(torch.maximum(ll, lu), torch.maximum(ul, uu))\n",
    "\n",
    "        return lower, upper\n",
    "\n",
    "\n",
    "    def left_multiply_by_constant_matrix(self, P_lower, P_upper, W):\n",
    "        \"\"\"\n",
    "        Batched version: Compute bounds on W @ P, given bounds on P.\n",
    "\n",
    "        Args:\n",
    "            P_lower, P_upper: (b, n, m)\n",
    "            W: (k, n) constant matrix\n",
    "\n",
    "        Returns:\n",
    "            (lower, upper): (b, k, m)\n",
    "        \"\"\"\n",
    "        # Use batch matrix multiply: (b, k, n) @ (b, n, m)\n",
    "        W_exp = W.unsqueeze(0).expand(P_lower.shape[0], -1, -1)  # (b, k, n)\n",
    "\n",
    "        X1 = torch.bmm(W_exp, P_lower)\n",
    "        X2 = torch.bmm(W_exp, P_upper)\n",
    "\n",
    "        lower = torch.minimum(X1, X2)\n",
    "        upper = torch.maximum(X1, X2)\n",
    "\n",
    "        return lower, upper\n",
    "\n",
    "    def compute_full_product_bound(self, diag_bounds_list, elision_matrix):\n",
    "        \"\"\"\n",
    "        Computes interval bounds on the product:\n",
    "        W_out · J_N · W_{N-1} · J_{N-1} · ... · J_2 · W_1\n",
    "\n",
    "        Batched version.\n",
    "\n",
    "        Args:\n",
    "            diag_bounds_list: list of (J_lower_i, J_upper_i), each of shape (b, n_i)\n",
    "                            length = N-1\n",
    "            elision_matrix: optional matrix to multiply with W_out at the end\n",
    "\n",
    "        Returns:\n",
    "            (M_lower, M_upper): tensors of shape (b, k, m) — bounds on the final matrix product\n",
    "        \"\"\"\n",
    "        batch_size = diag_bounds_list[0][0].shape[0]\n",
    "        assert len(diag_bounds_list) == len(self.hidden_layers), \\\n",
    "            f\"Expected {len(self.hidden_layers)} diag bounds but got {len(diag_bounds_list)}\"\n",
    "\n",
    "        # Step 1: Start from the rightmost matrix: W_1\n",
    "        W1 = self.hidden_layers[0].weight.clone()  # (n1, m1)\n",
    "        P_lower = W1.unsqueeze(0).expand(batch_size, -1, -1).clone()\n",
    "        P_upper = W1.unsqueeze(0).expand(batch_size, -1, -1).clone()\n",
    "\n",
    "        # Step 2: Loop through J_2, W_2, ..., J_N\n",
    "        for i in range(1, len(self.hidden_layers)):\n",
    "            # Multiply on the left with diag(J_i)\n",
    "            J_lower, J_upper = diag_bounds_list[i - 1]  # (b, n_i)\n",
    "            P_lower, P_upper = self.left_multiply_by_diag(J_lower, J_upper, P_lower, P_upper)\n",
    "\n",
    "            # Multiply on the left with constant W_i\n",
    "            W_i = self.hidden_layers[i].weight  # (n_{i+1}, n_i)\n",
    "            P_lower, P_upper = self.left_multiply_by_constant_matrix(P_lower, P_upper, W_i)\n",
    "\n",
    "        # Step 3: Final left multiplication with J_N\n",
    "        J_lower, J_upper = diag_bounds_list[-1]\n",
    "        P_lower, P_upper = self.left_multiply_by_diag(J_lower, J_upper, P_lower, P_upper)\n",
    "\n",
    "        # Step 4: Multiply by W_out\n",
    "        W_out = self.output_layer.weight.clone()  # (k, n_last)\n",
    "        if elision_matrix is not None:\n",
    "            W_out = elision_matrix @ W_out\n",
    "        final_lower, final_upper = self.left_multiply_by_constant_matrix(P_lower, P_upper, W_out)\n",
    "\n",
    "        return final_lower, final_upper\n",
    "\n",
    "    \n",
    "    def compute_Du_bounds(self, l, u, elision_matrix = None):\n",
    "        diag_bounds = self.get_diag_bounds(l, u)\n",
    "        return self.compute_full_product_bound(diag_bounds, elision_matrix)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "ad786f4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n0. Incorporate elision for constant contraction metric case -- DONE\\n1. Compute the interval bounds on the given vector field -- DONE\\n2. Computer upper bound on Metzlerization -- DONE\\n3. Check spectral abscissa -- DONE\\n4. ???\\n4\\n5. Partitioning\\n6. Neural contraction metrics\\n7. Training?\\n'"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "0. Incorporate elision for constant contraction metric case -- DONE\n",
    "1. Compute the interval bounds on the given vector field -- DONE\n",
    "2. Computer upper bound on Metzlerization -- DONE\n",
    "3. Check spectral abscissa -- DONE\n",
    "4. ???\n",
    "4\n",
    "5. Partitioning\n",
    "6. Neural contraction metrics\n",
    "7. Training?\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "463f8ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    # Example from Manchester and Slotine\n",
    "    return torch.tensor([-x[0] + x[2], \n",
    "    x[0]**2 - x[1] - 2*x[0]*x[2] + x[2], \n",
    "    -x[1]])\n",
    "\n",
    "def Df(x):\n",
    "    \"\"\"\n",
    "    Jacobian of vector field for batched input.\n",
    "    \n",
    "    Args:\n",
    "        x: Tensor of shape (batch_size, 3) or (3,)\n",
    "    Returns:\n",
    "        Jacobian: Tensor of shape (batch_size, 3, 3)\n",
    "    \"\"\"\n",
    "    # Ensure batch dimension\n",
    "    if x.ndim == 1:\n",
    "        x = x.unsqueeze(0)\n",
    "\n",
    "    batch_size = x.shape[0]\n",
    "    J = torch.zeros(batch_size, 3, 3, dtype=x.dtype, device=x.device)\n",
    "\n",
    "    # Fill in the Jacobian\n",
    "    J[:, 0, 0] = -1\n",
    "    J[:, 0, 1] = 0\n",
    "    J[:, 0, 2] = 1\n",
    "\n",
    "    J[:, 1, 0] = 2*x[:, 0] - 2*x[:, 2]\n",
    "    J[:, 1, 1] = -1\n",
    "    J[:, 1, 2] = -2*x[:, 0] + 1\n",
    "\n",
    "    J[:, 2, 0] = 0\n",
    "    J[:, 2, 1] = -1\n",
    "    J[:, 2, 2] = 0\n",
    "\n",
    "    return J\n",
    "\n",
    "def jac_bounds_old(l, u):\n",
    "    lower = torch.tensor([[-1, 0, 1],\n",
    "                         [2*l[0] - 2*u[2], -1, -2*u[0] + 1],\n",
    "                         [0, -1, 0]])\n",
    "    upper = torch.tensor([[-1, 0, 1],\n",
    "                         [2*u[0] - 2*l[2], -1, -2*l[0] + 1],\n",
    "                         [0, -1, 0]])\n",
    "    return lower, upper\n",
    "\n",
    "def jac_bounds(l, u):\n",
    "    # l, u: (B, 3) or (3,) if unbatched\n",
    "    # Ensure batching\n",
    "    if l.ndim == 1:\n",
    "        l = l.unsqueeze(0)\n",
    "        u = u.unsqueeze(0)\n",
    "    \n",
    "    B = l.shape[0]\n",
    "    device = l.device\n",
    "    dtype = l.dtype\n",
    "    \n",
    "    lower = torch.zeros((B, 3, 3), device=device, dtype=dtype)\n",
    "    upper = torch.zeros((B, 3, 3), device=device, dtype=dtype)\n",
    "    \n",
    "    # Fill in constant entries\n",
    "    lower[:, 0, 0] = -1\n",
    "    lower[:, 0, 2] = 1\n",
    "    lower[:, 1, 1] = -1\n",
    "    lower[:, 2, 1] = -1\n",
    "    \n",
    "    upper[:, 0, 0] = -1\n",
    "    upper[:, 0, 2] = 1\n",
    "    upper[:, 1, 1] = -1\n",
    "    upper[:, 2, 1] = -1\n",
    "    \n",
    "    # Fill in l/u dependent entries\n",
    "    lower[:, 1, 0] = 2*l[:, 0] - 2*u[:, 2]\n",
    "    lower[:, 1, 2] = -2*u[:, 0] + 1\n",
    "    \n",
    "    upper[:, 1, 0] = 2*u[:, 0] - 2*l[:, 2]\n",
    "    upper[:, 1, 2] = -2*l[:, 0] + 1\n",
    "    \n",
    "    return (lower, upper)\n",
    "\n",
    "\n",
    "B = torch.tensor([[0.], [0.], [1.]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "b8198398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 9.0691,  9.1878, 10.0146, 10.7423, 11.5907, 13.2548, 14.5605, 16.3539,\n",
      "        17.9352, 19.8787,  9.0885,  9.3671,  9.9398, 10.7449, 11.6573, 13.3223,\n",
      "        14.6798, 16.5641, 18.2927, 19.8363,  9.0664,  9.1802,  9.9918, 10.7825,\n",
      "        11.9010, 13.3930, 15.0150, 16.5984, 18.2230, 19.8478,  9.0151,  9.2138,\n",
      "         9.9752, 10.6831, 11.8484, 13.1630, 14.7053, 16.1453, 18.1696, 19.9150,\n",
      "         9.0463,  9.1408,  9.8848, 10.7690, 11.9339, 13.2602, 14.8534, 16.5440,\n",
      "        18.3227, 20.0607,  9.0256,  9.2502,  9.9027, 10.7717, 11.8574, 13.3975,\n",
      "        14.8944, 16.2522, 18.1111, 19.8769,  9.0513,  9.3059,  9.9977, 10.7358,\n",
      "        12.0265, 13.1615, 14.8238, 16.4350, 18.2426, 19.8370,  9.0557,  9.3422,\n",
      "         9.7147, 10.8961, 12.0380, 13.2317, 14.9370, 16.4558, 18.3238, 20.1273,\n",
      "         9.0509,  9.3063,  9.9120, 10.6774, 11.8450, 13.4018, 14.8779, 16.4896,\n",
      "        18.0304, 20.0296,  9.0631,  9.3564,  9.9560, 10.8376, 11.9053, 13.3248,\n",
      "        14.8760, 16.5928, 18.2160, 19.9063,  7.2702,  7.0525,  7.4565,  8.1989,\n",
      "         9.0200, 10.5540, 12.0637, 13.5333, 15.1885, 17.4809,  7.4614,  7.0551,\n",
      "         7.3712,  8.1919,  9.2147, 10.4279, 12.1946, 13.6884, 15.5365, 17.2849,\n",
      "         7.4413,  7.0558,  7.3234,  8.1845,  9.0847, 10.6169, 11.9980, 13.6256,\n",
      "        15.5441, 17.3006,  7.4297,  7.0450,  7.4174,  8.1367,  9.1414, 10.6524,\n",
      "        12.0564, 13.8344, 15.3579, 16.9584,  7.4446,  7.0573,  7.2900,  8.1015,\n",
      "         9.2398, 10.3953, 12.2073, 13.5666, 15.3890, 17.2270,  7.4371,  7.0710,\n",
      "         7.3595,  8.0028,  9.0971, 10.5201, 12.0312, 13.4822, 15.2893, 16.9573,\n",
      "         7.3350,  7.0123,  7.4145,  8.1337,  9.1992, 10.5382, 12.1632, 13.7678,\n",
      "        15.4384, 17.4575,  7.3638,  7.0516,  7.4390,  8.0738,  9.0278, 10.4517,\n",
      "        12.0274, 13.8768, 15.2751, 17.1602,  7.3566,  7.0478,  7.3261,  7.9599,\n",
      "         8.9220, 10.4289, 12.1422, 13.5886, 15.0345, 16.9746,  7.4942,  7.0484,\n",
      "         7.2316,  8.0222,  8.9457, 10.5974, 11.8804, 13.7860, 15.2090, 17.0281,\n",
      "         6.3716,  5.5776,  5.0892,  5.3864,  6.1156,  7.6506,  9.1473, 10.8089,\n",
      "        12.6002, 14.6577,  6.3611,  5.5762,  5.0965,  5.3962,  6.5427,  7.5565,\n",
      "         9.3623, 11.0859, 12.9301, 14.6675,  6.3745,  5.5058,  5.0915,  5.3442,\n",
      "         6.3884,  7.6508,  9.4363, 10.8817, 12.6414, 14.5733,  6.4042,  5.5276,\n",
      "         5.1338,  5.3233,  6.3156,  7.4159,  9.2125, 10.8000, 12.8111, 14.6133,\n",
      "         6.3432,  5.5083,  5.1067,  5.2971,  6.3617,  7.5046,  9.4693, 10.9155,\n",
      "        12.5640, 14.4815,  6.3311,  5.6304,  5.0579,  5.5408,  6.4117,  7.6492,\n",
      "         9.1540, 10.8166, 12.5637, 14.7315,  6.3642,  5.4995,  5.1100,  5.3917,\n",
      "         6.4083,  7.6234,  9.2089, 10.9423, 12.9609, 14.6917,  6.4536,  5.5875,\n",
      "         5.0712,  5.3021,  6.4199,  7.5284,  9.1817, 10.9809, 12.9047, 14.7113,\n",
      "         6.4266,  5.5839,  5.0879,  5.3642,  6.3146,  7.7508,  9.1632, 10.9456,\n",
      "        12.5932, 14.4763,  6.2109,  5.5212,  5.0958,  5.5595,  6.3747,  7.8300,\n",
      "         9.4611, 10.7310, 12.6796, 14.4308,  6.5438,  4.8500,  3.6656,  3.1432,\n",
      "         3.6567,  4.6806,  6.6123,  8.4199, 10.0187, 12.3124,  6.4023,  4.8112,\n",
      "         3.7208,  3.1603,  3.5673,  4.8973,  6.4596,  8.2061, 10.4126, 12.1336,\n",
      "         6.4556,  4.7950,  3.7210,  3.2135,  3.4815,  4.7981,  6.6329,  8.2857,\n",
      "        10.1261, 12.1673,  6.5054,  4.7384,  3.8021,  3.1472,  3.5648,  4.7242,\n",
      "         6.3440,  8.3606, 10.1216, 12.0579,  6.4352,  4.8623,  3.7612,  3.1150,\n",
      "         3.5436,  4.5374,  6.2303,  8.5061, 10.0559, 12.1322,  6.5286,  4.8051,\n",
      "         3.7293,  3.1652,  3.6071,  4.8848,  6.1015,  8.5151, 10.2040, 11.9711,\n",
      "         6.6173,  4.9064,  3.7041,  3.1144,  3.7147,  4.7261,  6.2895,  8.2748,\n",
      "        10.1063, 12.2085,  6.3480,  4.7911,  3.5417,  3.1408,  3.4250,  5.0011,\n",
      "         6.3582,  8.3397, 10.2693, 11.8486,  6.4116,  4.7445,  3.7899,  3.1157,\n",
      "         3.6626,  4.4646,  6.5393,  8.5068, 10.3389, 11.8584,  6.6023,  4.7697,\n",
      "         3.6189,  3.1540,  3.2242,  4.5839,  6.6583,  8.4180, 10.0543, 12.0278,\n",
      "         7.7790,  5.9216,  3.9937,  2.0369,  1.4027,  1.9357,  4.0913,  5.9578,\n",
      "         7.9306,  9.7249,  7.9183,  5.7484,  4.0393,  2.2071,  1.3534,  2.1650,\n",
      "         3.6045,  5.5367,  7.7596,  9.6660,  7.7730,  5.9582,  4.0070,  2.1585,\n",
      "         1.3529,  1.9771,  3.8764,  5.4975,  7.6872,  9.8722,  7.8886,  5.8450,\n",
      "         3.8850,  2.1438,  1.3624,  1.8954,  3.8284,  5.7233,  7.9274,  9.6175,\n",
      "         7.9398,  5.7832,  3.9997,  2.1067,  1.3754,  2.1136,  3.9966,  5.9826,\n",
      "         7.5307,  9.8980,  7.9376,  5.9720,  3.8389,  2.1416,  1.3945,  1.9088,\n",
      "         3.9770,  5.8678,  7.8508,  9.8231,  7.8286,  5.9786,  3.5837,  2.0916,\n",
      "         1.3696,  2.0145,  3.7430,  5.8959,  7.9525,  9.8446,  7.8794,  6.0651,\n",
      "         3.8655,  2.1753,  1.3336,  1.8001,  3.6084,  6.0207,  7.8162,  9.9247,\n",
      "         7.7663,  5.7981,  3.8347,  2.0843,  1.2653,  1.7893,  4.0067,  5.7601,\n",
      "         7.9373,  9.9245,  7.8459,  5.8137,  3.9107,  2.0917,  1.3540,  2.1587,\n",
      "         3.6783,  5.7165,  7.6549,  9.8513,  9.7619,  7.8258,  5.9403,  3.6976,\n",
      "         1.8329,  1.3291,  2.1652,  3.9049,  6.0052,  7.8617,  9.9347,  7.6822,\n",
      "         5.9797,  3.7258,  2.2446,  1.3702,  2.2159,  4.0769,  5.8977,  7.7053,\n",
      "         9.8337,  7.7486,  5.7950,  3.7056,  2.0338,  1.2764,  2.0920,  3.9187,\n",
      "         5.9493,  7.8526,  9.7262,  7.6989,  6.0044,  4.0837,  2.0034,  1.4441,\n",
      "         2.1706,  3.8993,  5.9091,  7.7715,  9.9799,  7.9039,  5.8619,  3.9466,\n",
      "         2.2572,  1.3760,  2.1361,  3.7061,  5.9505,  7.8944,  9.8698,  7.8367,\n",
      "         5.6775,  3.8279,  2.2741,  1.3691,  2.1314,  4.0105,  5.9674,  7.8786,\n",
      "         9.9420,  8.0457,  5.7265,  3.8601,  1.8750,  1.3754,  2.0734,  4.0875,\n",
      "         5.5954,  7.9540,  9.8516,  7.8843,  5.5555,  3.8756,  2.0248,  1.3845,\n",
      "         2.1704,  3.8660,  5.9442,  7.9008,  9.7058,  7.9825,  5.9592,  3.4877,\n",
      "         2.1757,  1.2745,  2.0939,  3.9476,  5.6725,  7.9660, 10.0085,  7.7734,\n",
      "         5.8283,  3.9991,  2.0486,  1.3011,  2.0723,  3.8820,  5.9701,  7.7181,\n",
      "        12.2836, 10.2751,  8.4479,  6.6676,  4.9610,  3.5252,  3.2315,  3.7493,\n",
      "         4.7650,  6.5125, 12.2255, 10.3090,  8.1940,  6.3993,  4.8158,  3.7043,\n",
      "         3.1647,  3.7757,  4.7591,  6.4931, 12.2312, 10.3019,  8.4437,  6.3219,\n",
      "         4.9462,  3.4566,  3.2246,  3.7980,  4.7547,  6.5090, 12.2644, 10.1806,\n",
      "         8.2061,  6.5914,  4.6441,  3.5202,  3.1552,  3.8104,  4.8010,  6.3595,\n",
      "        12.1341, 10.1274,  8.2808,  6.4803,  4.6743,  3.6303,  3.1528,  3.6923,\n",
      "         4.8911,  6.4550, 12.1693, 10.3230,  8.0096,  6.3393,  4.6728,  3.6449,\n",
      "         3.1765,  3.7907,  4.7832,  6.5774, 11.9942, 10.1909,  8.1981,  6.3131,\n",
      "         4.8135,  3.6194,  3.2235,  3.6021,  4.8196,  6.5004, 12.2314, 10.3207,\n",
      "         8.5129,  6.1509,  4.5915,  3.5435,  3.1360,  3.6976,  4.8653,  6.4236,\n",
      "        12.2151, 10.1571,  8.2530,  6.5981,  4.7881,  3.5369,  3.1578,  3.7616,\n",
      "         4.9105,  6.6094, 11.9427,  9.9492,  8.3166,  6.3452,  5.0189,  3.5219,\n",
      "         3.1872,  3.7655,  4.8389,  6.5439, 14.5616, 12.8045, 11.1686,  9.2330,\n",
      "         7.8942,  6.2305,  5.3717,  5.1081,  5.5600,  6.3659, 14.8067, 12.9802,\n",
      "        10.8610,  8.9630,  7.6091,  6.2142,  5.2614,  5.0699,  5.5461,  6.3655,\n",
      "        14.1710, 12.6458, 11.1562,  9.2040,  7.5772,  6.4389,  5.3243,  5.1333,\n",
      "         5.5820,  6.3340, 14.4644, 12.6597, 10.9750,  9.2572,  7.6859,  6.2252,\n",
      "         5.3365,  5.1135,  5.5471,  6.3729, 14.6238, 12.8695, 10.7690,  9.1887,\n",
      "         7.7694,  6.3306,  5.4749,  5.0569,  5.6010,  6.4055, 14.5945, 12.8733,\n",
      "        11.0777,  9.4687,  7.4766,  6.5477,  5.3675,  5.1056,  5.5803,  6.3853,\n",
      "        14.5249, 12.8924, 11.0447,  9.4147,  7.4674,  6.2894,  5.2750,  5.0968,\n",
      "         5.5214,  6.3737, 14.6748, 12.6355, 10.9655,  9.3698,  7.6899,  6.3805,\n",
      "         5.4390,  5.0929,  5.5777,  6.3666, 14.4936, 12.9071, 10.9403,  9.2332,\n",
      "         7.8434,  6.4226,  5.2974,  5.0807,  5.6129,  6.2892, 14.6115, 12.9310,\n",
      "        10.8050,  9.2296,  7.3951,  6.3521,  5.3340,  5.1072,  5.5240,  6.3995,\n",
      "        17.0948, 15.5011, 13.5087, 11.8934, 10.5295,  9.2176,  8.0455,  7.3791,\n",
      "         7.1061,  7.3265, 17.2355, 15.5196, 13.6505, 11.9241, 10.3239,  9.2278,\n",
      "         8.1091,  7.3058,  7.0523,  7.4331, 17.4004, 15.6524, 13.9047, 12.0337,\n",
      "        10.4864,  8.9205,  8.1514,  7.3484,  7.0784,  7.4674, 17.4240, 15.5918,\n",
      "        13.5436, 12.0068, 10.4609,  9.2123,  8.2282,  7.3415,  7.0656,  7.4109,\n",
      "        17.2267, 15.3992, 13.6889, 11.9947, 10.2863,  9.2770,  8.0885,  7.3220,\n",
      "         7.1119,  7.5008, 17.3847, 15.4535, 13.4940, 11.8531, 10.3987,  9.1217,\n",
      "         8.1695,  7.4152,  7.0566,  7.3757, 17.4013, 15.4578, 13.7078, 12.1667,\n",
      "        10.6021,  9.1555,  8.0896,  7.3484,  7.0969,  7.3756, 16.9233, 15.5872,\n",
      "        13.7371, 12.1269, 10.6378,  9.2494,  8.1592,  7.1872,  7.0225,  7.3254,\n",
      "        16.9698, 15.2469, 13.6452, 12.2104, 10.5390,  9.2971,  7.9702,  7.3632,\n",
      "         7.0443,  7.3241, 17.4519, 15.5331, 13.6964, 12.1628, 10.4878,  8.9688,\n",
      "         8.1911,  7.3945,  7.1065,  7.4309, 19.9775, 17.7953, 16.4922, 15.0233,\n",
      "        13.3776, 11.8522, 10.8916,  9.9640,  9.3141,  9.0138, 19.7665, 17.8868,\n",
      "        16.6525, 14.8568, 13.2408, 11.8689, 10.9621, 10.0136,  9.1660,  9.0307,\n",
      "        20.0715, 18.3269, 16.4401, 14.8932, 13.4905, 11.7408, 10.5353,  9.7421,\n",
      "         9.3061,  9.0237, 19.8253, 18.0814, 16.5485, 14.7773, 13.3967, 12.0635,\n",
      "        10.8666,  9.8388,  9.3508,  9.0542, 19.7816, 17.9563, 16.5645, 14.9546,\n",
      "        13.1762, 11.9858, 10.9098,  9.9062,  9.2486,  9.0488, 19.7255, 17.9445,\n",
      "        16.2712, 14.7589, 13.1490, 12.0549, 10.7558,  9.8743,  9.2599,  9.0455,\n",
      "        19.4922, 17.9517, 16.5430, 14.8470, 13.1353, 11.9617, 10.8228,  9.8621,\n",
      "         9.3330,  9.0740, 20.0138, 17.9316, 16.3493, 14.9541, 13.3564, 11.8654,\n",
      "        10.8217,  9.8178,  9.3122,  9.0417, 19.9952, 17.8812, 16.4956, 14.9587,\n",
      "        13.1340, 11.9529, 10.9612,  9.6104,  9.2485,  9.0667, 19.7784, 18.1982,\n",
      "        16.3791, 14.7099, 13.4324, 11.9375, 10.7817,  9.8375,  9.3118,  9.0130])\n"
     ]
    }
   ],
   "source": [
    "def max_eig_over_hyperrectangles(J_func, l, u, P, num_samples=100):\n",
    "    \"\"\"\n",
    "    Compute the maximum eigenvalue of P @ J(x) + J(x).T @ P\n",
    "    over random samples inside each hyperrectangle.\n",
    "\n",
    "    Args:\n",
    "        J_func: callable, takes (..., dim) tensor and returns (..., n, n) Jacobians.\n",
    "        bounds: tensor of shape (batch, dim, 2) giving [min, max] for each dim.\n",
    "        P: (n, n) symmetric matrix.\n",
    "        num_samples: number of samples per hyperrectangle.\n",
    "\n",
    "    Returns:\n",
    "        max_eigs: tensor of shape (batch,) with max eigenvalue per hyperrectangle.\n",
    "    \"\"\"\n",
    "    batch_size, dim = l.shape\n",
    "    device = l.device\n",
    "    n = P.shape[0]\n",
    "\n",
    "    # Sample uniformly in each hyperrectangle\n",
    "    rand = torch.rand(batch_size, num_samples, dim, device=device)\n",
    "    samples = l[:, None, :] + rand * (u - l)[:, None, :]  # (batch, num_samples, dim)\n",
    "\n",
    "    # Flatten samples for batch processing\n",
    "    flat_samples = samples.reshape(-1, dim)  # (batch * num_samples, dim)\n",
    "\n",
    "    # Evaluate J(x) for all samples\n",
    "    J_vals = J_func(flat_samples)  # (batch*num_samples, n, n)\n",
    "\n",
    "    # Compute PJ + J^T P\n",
    "    PJ = torch.matmul(P, J_vals)  # (batch*num_samples, n, n)\n",
    "    JTP = torch.matmul(J_vals.transpose(-1, -2), P)\n",
    "    M = PJ + JTP  # (batch*num_samples, n, n)\n",
    "\n",
    "    # Compute largest eigenvalue for each matrix\n",
    "    eigvals = torch.linalg.eigvalsh(M)  # (..., n)\n",
    "    max_eigs_per_sample = eigvals[..., -1]  # (batch*num_samples,)\n",
    "\n",
    "    # Reshape back to (batch, num_samples) and take max over samples\n",
    "    max_eigs = max_eigs_per_sample.view(batch_size, num_samples).max(dim=1).values\n",
    "\n",
    "    return max_eigs\n",
    "\n",
    "P = torch.eye(3)\n",
    "print(max_eig_over_hyperrectangles(Df, xunder, xover, P))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "df48b455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "NN_IBP(\n",
      "  (hidden_layers): ModuleList(\n",
      "    (0): Linear(in_features=3, out_features=16, bias=True)\n",
      "    (1): Linear(in_features=16, out_features=16, bias=True)\n",
      "  )\n",
      "  (activations): ModuleList(\n",
      "    (0-1): 2 x Softplus(beta=1.0, threshold=20.0)\n",
      "  )\n",
      "  (P): Linear(in_features=3, out_features=3, bias=False)\n",
      "  (output_layer): Linear(in_features=16, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nWant bound on P @ Df + Df^T P + P B Du + (P B Du)^T\\n'"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_metzler_upper_bound(jac_bounds, controller_bounds):\n",
    "    jac_lower, jac_upper = jac_bounds       # (B, n, n)\n",
    "    Du_lower, Du_upper = controller_bounds  # (B, n, n)\n",
    "    \n",
    "    # Transpose only the last two dims for batching\n",
    "    jac_lower_T = jac_lower.transpose(-1, -2)\n",
    "    jac_upper_T = jac_upper.transpose(-1, -2)\n",
    "    Du_lower_T = Du_lower.transpose(-1, -2)\n",
    "    Du_upper_T = Du_upper.transpose(-1, -2)\n",
    "    \n",
    "    # Compute LMI lower and upper bounds (batch-wise)\n",
    "    lmi_lower = 0.5 * (jac_lower + jac_lower_T + Du_lower + Du_lower_T)\n",
    "    lmi_upper = 0.5 * (jac_upper + jac_upper_T + Du_upper + Du_upper_T)\n",
    "    \n",
    "    # Elementwise maximum for the Metzler bound\n",
    "    mat_abs = torch.maximum(lmi_upper, -lmi_lower)\n",
    "    \n",
    "    # Extract diagonals in batched way\n",
    "    diag_mat_abs = torch.diagonal(mat_abs, dim1=-2, dim2=-1)\n",
    "    diag_lmi_upper = torch.diagonal(lmi_upper, dim1=-2, dim2=-1)\n",
    "    \n",
    "    # Zero out diagonal of mat_abs, then replace with lmi_upper diagonal\n",
    "    result = mat_abs - torch.diag_embed(diag_mat_abs) + torch.diag_embed(diag_lmi_upper)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def compute_metzler_upper_bound_new(jac_eigenbounds, controller_bounds):\n",
    "    #jac_lower, jac_upper = jac_bounds       # (B, n, n)\n",
    "    Du_lower, Du_upper = controller_bounds  # (B, n, n)\n",
    "    \n",
    "    # Transpose only the last two dims for batching\n",
    "    #jac_lower_T = jac_lower.transpose(-1, -2)\n",
    "    #jac_upper_T = jac_upper.transpose(-1, -2)\n",
    "    Du_lower_T = Du_lower.transpose(-1, -2)\n",
    "    Du_upper_T = Du_upper.transpose(-1, -2)\n",
    "    \n",
    "    # Compute LMI lower and upper bounds (batch-wise)\n",
    "    lmi_lower = (Du_lower + Du_lower_T)\n",
    "    lmi_upper = (Du_upper + Du_upper_T)\n",
    "    \n",
    "    # Elementwise maximum for the Metzler bound\n",
    "    mat_abs = torch.maximum(lmi_upper, -lmi_lower)\n",
    "    \n",
    "    # Extract diagonals in batched way\n",
    "    diag_mat_abs = torch.diagonal(mat_abs, dim1=-2, dim2=-1)\n",
    "    diag_lmi_upper = torch.diagonal(lmi_upper, dim1=-2, dim2=-1)\n",
    "    \n",
    "    # Zero out diagonal of mat_abs, then replace with lmi_upper diagonal\n",
    "    result = mat_abs - torch.diag_embed(diag_mat_abs) + torch.diag_embed(diag_lmi_upper) + jac_eigenbounds[:, None, None] * torch.eye(Du_upper.shape[-1])\n",
    "    print(result[0])\n",
    "    return result\n",
    "\n",
    "\n",
    "def max_eig_metzler(M, num_iter=200, tol=1e-8):\n",
    "    \"\"\"\n",
    "    Compute max eigenvalue of batched Metzler matrices M using power iteration.\n",
    "\n",
    "    Args:\n",
    "        M: (b, n, n) batched Metzler matrices\n",
    "        num_iter: max iterations\n",
    "        tol: convergence tolerance\n",
    "\n",
    "    Returns:\n",
    "        lambda_max: (b,) largest eigenvalues per batch\n",
    "    \"\"\"\n",
    "    bshape = M.shape[:-2]\n",
    "    n = M.shape[-1]\n",
    "    v = torch.ones(*bshape, n, dtype=M.dtype, device=M.device)\n",
    "    v = v / v.sum(dim=-1, keepdim=True)\n",
    "\n",
    "    for _ in range(num_iter):\n",
    "        v_next = torch.matmul(M, v.unsqueeze(-1)).squeeze(-1)\n",
    "        norm = v_next.sum(dim=-1, keepdim=True)\n",
    "        v_next = v_next / (norm + 1e-12)\n",
    "        v_next = torch.clamp(v_next, min=1e-8)\n",
    "        \n",
    "        # Convergence check (L1 difference)\n",
    "        if torch.max(torch.abs(v_next - v)) < tol:\n",
    "            break\n",
    "        v = v_next\n",
    "\n",
    "    # Rayleigh quotient estimate\n",
    "    numerator = torch.sum(v * torch.matmul(M, v.unsqueeze(-1)).squeeze(-1), dim=-1)\n",
    "    denominator = torch.sum(v * v, dim=-1)\n",
    "    lambda_max = numerator / (denominator + 1e-12)\n",
    "    return lambda_max\n",
    "\n",
    "def smooth_relu(x, delta=1.0):\n",
    "    # x: tensor\n",
    "    # delta: smoothing interval width > 0\n",
    "\n",
    "    zero = torch.zeros_like(x)\n",
    "    # mask regions\n",
    "    mask_neg = (x <= 0)\n",
    "    mask_smooth = (x > 0) & (x < delta)\n",
    "    mask_linear = (x >= delta)\n",
    "\n",
    "    a = -2.0 / (delta ** 3)\n",
    "    b = 3.0 / (delta ** 2)\n",
    "\n",
    "    # Compute smooth cubic part\n",
    "    smooth_part = a * x**3 + b * x**2\n",
    "\n",
    "    return torch.where(\n",
    "        mask_neg, zero,\n",
    "        torch.where(\n",
    "            mask_smooth, smooth_part,\n",
    "            x  # linear region\n",
    "        )\n",
    "    )\n",
    "\n",
    "def max_eig_metzler_shifted(M, num_iter=50, tol=1e-4):\n",
    "    \"\"\"\n",
    "    Compute the max eigenvalue of a batched symmetric Metzler matrix M using power iteration,\n",
    "    with spectral shift to ensure nonnegativity for stable convergence.\n",
    "\n",
    "    Args:\n",
    "        M: (b, n, n) batched symmetric Metzler matrices\n",
    "        num_iter: maximum power iteration steps\n",
    "        tol: convergence tolerance\n",
    "\n",
    "    Returns:\n",
    "        lambda_max: (b,) largest eigenvalue estimates per batch\n",
    "    \"\"\"\n",
    "    bshape = M.shape[:-2]\n",
    "    n = M.shape[-1]\n",
    "    device = M.device\n",
    "    dtype = M.dtype\n",
    "\n",
    "    # Compute the shift scalar: at least -min diagonal entry, plus small margin\n",
    "    # Since M is Metzler, off-diagonal >= 0 but diagonals can be negative\n",
    "    min_diag, _ = torch.min(torch.diagonal(M, dim1=-2, dim2=-1), dim=-1, keepdim=True)  # (b,1)\n",
    "    shift = -min_diag + 1e-3  # small positive margin\n",
    "    shift = shift.unsqueeze(-1)  # shape (b,1,1) for broadcasting\n",
    "\n",
    "    # Shift the matrix: M_shifted = M + shift * I\n",
    "    I = torch.eye(n, device=device, dtype=dtype).unsqueeze(0).expand(*bshape, n, n)\n",
    "    M_shifted = M + shift * I\n",
    "\n",
    "    # Initialize positive starting vector (batch)\n",
    "    v = torch.ones(*bshape, n, device=device, dtype=dtype)\n",
    "    v = v / v.sum(dim=-1, keepdim=True)\n",
    "\n",
    "    for _ in range(num_iter):\n",
    "        v_next = torch.matmul(M_shifted, v.unsqueeze(-1)).squeeze(-1)\n",
    "        norm = v_next.sum(dim=-1, keepdim=True)\n",
    "        v_next = v_next / (norm + 1e-12)\n",
    "        v_next = torch.clamp(v_next, min=1e-8)\n",
    "        \n",
    "        if torch.max(torch.abs(v_next - v)) < tol:\n",
    "            break\n",
    "        v = v_next\n",
    "\n",
    "    # Rayleigh quotient for shifted matrix\n",
    "    numerator = torch.sum(v * torch.matmul(M_shifted, v.unsqueeze(-1)).squeeze(-1), dim=-1)\n",
    "    denominator = torch.sum(v * v, dim=-1)\n",
    "    lambda_max_shifted = numerator / (denominator + 1e-12)\n",
    "\n",
    "    # Subtract shift to get original max eigenvalue\n",
    "    lambda_max = lambda_max_shifted - shift.squeeze(-1).squeeze(-1)\n",
    "\n",
    "    return lambda_max\n",
    "\n",
    "def partition_hyperrectangle(xunder, xover, n_parts):\n",
    "    \"\"\"\n",
    "    Partition a hyperrectangle [xunder, xover] into n_parts equally sized sub-rectangles.\n",
    "\n",
    "    Args:\n",
    "        xunder (torch.Tensor): Lower corner, shape (d,)\n",
    "        xover (torch.Tensor): Upper corner, shape (d,)\n",
    "        n_parts (int): Number of partitions (must be a^d for some integer a)\n",
    "\n",
    "    Returns:\n",
    "        sub_xunder (torch.Tensor): Lower corners of sub-rectangles, shape (n_parts, d)\n",
    "        sub_xover (torch.Tensor): Upper corners of sub-rectangles, shape (n_parts, d)\n",
    "    \"\"\"\n",
    "    dim = xunder.shape[0]\n",
    "    # Determine number of splits along each axis\n",
    "    splits_per_dim = round(n_parts ** (1.0 / dim))\n",
    "    assert splits_per_dim ** dim == n_parts, \\\n",
    "        \"n_parts must be a perfect power of the number of dimensions\"\n",
    "\n",
    "    # Create equally spaced points along each dimension\n",
    "    edges = [\n",
    "        torch.linspace(xunder[i], xover[i], splits_per_dim + 1)\n",
    "        for i in range(dim)\n",
    "    ]\n",
    "\n",
    "    # Lower and upper bounds for each small rectangle\n",
    "    lower_grid = torch.stack(torch.meshgrid(*[e[:-1] for e in edges], indexing='ij'), dim=-1)\n",
    "    upper_grid = torch.stack(torch.meshgrid(*[e[1:] for e in edges], indexing='ij'), dim=-1)\n",
    "\n",
    "    sub_xunder = lower_grid.reshape(-1, dim)\n",
    "    sub_xover = upper_grid.reshape(-1, dim)\n",
    "\n",
    "    return sub_xunder, sub_xover\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "xunder = torch.tensor([0.0, 0.0, 0.0])\n",
    "xover = torch.tensor([1.0, 2.0, 3.0])\n",
    "n_parts = 64  # 4^3\n",
    "\n",
    "sub_xunder, sub_xover = partition_hyperrectangle(xunder, xover, n_parts)\n",
    "\n",
    "print(sub_xunder.shape)  # (64, 3)\n",
    "print(sub_xover.shape)   # (64, 3)\n",
    "\n",
    "\n",
    "model = NN_IBP()\n",
    "print(model)\n",
    "\n",
    "xunder = -0.5*torch.ones(3)\n",
    "xover = -xunder\n",
    "\n",
    "diag_bounds = model.get_diag_bounds(xunder, xover)\n",
    "#print(diag_bounds)\n",
    "\n",
    "Du_lower, Du_upper = model.compute_Du_bounds(xunder, xover, elision_matrix=B)\n",
    "jac_lower, jac_upper = jac_bounds(xunder, xover)\n",
    "\n",
    "'''\n",
    "Want bound on P @ Df + Df^T P + P B Du + (P B Du)^T\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "1c0926fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1000/20000] Loss: 458.7473 \n",
      "Epoch [2000/20000] Loss: 125.9778 \n",
      "Epoch [3000/20000] Loss: 45.9854 \n",
      "Epoch [4000/20000] Loss: 17.1123 \n",
      "Epoch [5000/20000] Loss: 4.9209 \n",
      "Epoch [6000/20000] Loss: 2.6296 \n",
      "Epoch [7000/20000] Loss: 0.1641 \n",
      "At epoch  7079  loss has hit 0, valid closed-loop contracting controller\n",
      "Metzler upper bound: tensor([[-4.1635e+02,  2.5244e+00,  6.9257e+00],\n",
      "        [ 2.5244e+00, -1.0125e+00,  6.2705e+00],\n",
      "        [ 6.9257e+00,  6.2705e+00, -1.2930e+03]], grad_fn=<SelectBackward0>)\n",
      "tensor([[-1.2931e+03, -4.1631e+02, -9.6616e-01],\n",
      "        [-1.2948e+03, -4.1612e+02, -9.3835e-01],\n",
      "        [-1.3016e+03, -4.1592e+02, -8.9214e-01],\n",
      "        ...,\n",
      "        [-1.5260e+03, -4.1657e+02, -8.4518e-01],\n",
      "        [-1.5204e+03, -4.1621e+02, -8.8291e-01],\n",
      "        [-1.5132e+03, -4.1592e+02, -8.9467e-01]],\n",
      "       grad_fn=<LinalgEighBackward0>)\n",
      "contraction metric tensor([[ 4.0043e+02, -2.5413e-02,  2.5701e-01],\n",
      "        [-2.5413e-02,  1.0000e+00,  1.1672e-02],\n",
      "        [ 2.5701e-01,  1.1672e-02,  5.1575e+00]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "eps = 1e-8\n",
    "learning_rate = 1e-2\n",
    "weight_decay = 1e-4  # L2 regularization\n",
    "num_epochs = 20000\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "xunder = -10*torch.ones(3)\n",
    "eye = torch.eye(3).to(device=device)\n",
    "xover = 10*torch.ones(3)\n",
    "\n",
    "xunder, xover = partition_hyperrectangle(xunder, xover, 10**3)\n",
    "\n",
    "# --- Send model to device ---\n",
    "model = NN_IBP(hidden_dims=[32,32], trainable_NCM=True)\n",
    "model = model.to(device)\n",
    "\n",
    "# --- Loss and optimizer ---\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# --- Training loop ---\n",
    "for epoch in range(num_epochs):\n",
    "    # ---- Training ----\n",
    "    model.train()\n",
    "    # Check this function for bugs\n",
    "    NCM = model.constant_NCM()\n",
    "    Du_bounds = model.compute_Du_bounds(xunder, xover, elision_matrix=NCM @ B)\n",
    "\n",
    "    #print(Du_bounds[0].shape)\n",
    "    Df_lower, Df_upper = jac_bounds(xunder, xover)\n",
    "    Df_lower, Df_upper = model.left_multiply_by_constant_matrix(Df_lower, Df_upper, NCM)\n",
    "    #print(Df_bounds[0].shape)\n",
    "    # eigenbounds = max_eig_over_hyperrectangles(Df, xunder, xover, NCM)\n",
    "    B_Mzr = compute_metzler_upper_bound((Df_lower, Df_upper), Du_bounds)\n",
    "    # B_Mzr = compute_metzler_upper_bound_new(eigenbounds, Du_bounds)\n",
    "    # improve conditioning\n",
    "    B_Mzr = B_Mzr + eps * torch.eye(B_Mzr.shape[-1], device=B_Mzr.device).unsqueeze(0)\n",
    "    try:\n",
    "        eigs = torch.linalg.eigvalsh(B_Mzr)\n",
    "    except:\n",
    "        print('poor conditioning')\n",
    "        print(B_Mzr[0])\n",
    "        \n",
    "        break\n",
    "    max_eig = eigs.amax(dim=tuple(range(1, eigs.ndim)))\n",
    "    # max_eig = max_eig_metzler_shifted(B_Mzr)\n",
    "    # max_eig = B_Mzr.sum(dim=-1)\n",
    "    # print(max_eig)\n",
    "    loss = torch.sum(torch.relu(max_eig))\n",
    "    if torch.isnan(loss):\n",
    "        print('NaN detected in loss')\n",
    "        break\n",
    "\n",
    "    if loss <= 1e-7:\n",
    "        print('At epoch ', epoch+1, ' loss has hit 0, valid closed-loop contracting controller')\n",
    "        print('Metzler upper bound:', B_Mzr[0])\n",
    "        print(torch.linalg.eigvalsh(B_Mzr))\n",
    "        break\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None and torch.isnan(param.grad).any():\n",
    "            print(f\"NaN detected in gradients of {name}\")\n",
    "\n",
    "    if epoch % 1000 == 999:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "              f\"Loss: {loss:.4f} \")\n",
    "        \n",
    "\n",
    "print('contraction metric', model.constant_NCM())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "29dd6bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "xunder = -10.0*torch.ones(3)\n",
    "xover = 10.0*torch.ones(3)\n",
    "\n",
    "NCM = model.constant_NCM()\n",
    "\n",
    "xunder, xover = partition_hyperrectangle(xunder, xover, 10**3)\n",
    "\n",
    "Du_bounds = model.compute_Du_bounds(xunder, xover, elision_matrix=NCM @ B)\n",
    "Df_lower, Df_upper = jac_bounds(xunder, xover)\n",
    "Df_lower, Df_upper = model.left_multiply_by_constant_matrix(Df_lower, Df_upper, NCM)\n",
    "B_Mzr = compute_metzler_upper_bound((Df_lower, Df_upper), Du_bounds)\n",
    "\n",
    "\n",
    "B_Mzr = compute_metzler_upper_bound((Df_lower, Df_upper), Du_bounds)\n",
    "loss = torch.relu(torch.max(torch.linalg.eigvalsh(B_Mzr)))\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "6c31a6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo:\n",
    "# 1. Handle arbitrary constant P by adjusting the method for computing bounds on Df (done!)\n",
    "# 2. Get 1 nice example working (pendulum maybe)\n",
    "# 3. Write code for general contraction metrics M(x) - perhaps another class for NCM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
