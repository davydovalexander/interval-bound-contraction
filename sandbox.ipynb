{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb8c1b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "[ 0.01783945  0.01771104 -0.00116054]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x16a137560>]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cvxpy as cp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def drift(x):\n",
    "    return np.array([-x[0] + x[2], \n",
    "    x[0]**2 - x[1] - 2*x[0]*x[2] + x[2], \n",
    "    -x[1]])\n",
    "\n",
    "def NCM(x):\n",
    "    return np.eye(3)\n",
    "\n",
    "def smooth_relu(x):\n",
    "    return np.log(1 + np.exp(x))\n",
    "\n",
    "def multiply_diagonal(W, L_diag, U_diag, side='right'):\n",
    "    \"\"\"\n",
    "    Multiply matrix W with diagonal interval matrix [L_diag, U_diag].\n",
    "    \n",
    "    If side = 'right', returns bounds for W * J\n",
    "    If side = 'left',  returns bounds for J * W\n",
    "    \"\"\"\n",
    "    L_diag = np.asarray(L_diag).reshape(-1)\n",
    "    U_diag = np.asarray(U_diag).reshape(-1)\n",
    "\n",
    "    if side == 'right':\n",
    "        prod1 = W * L_diag  # broadcasting\n",
    "        prod2 = W * U_diag\n",
    "    elif side == 'left':\n",
    "        prod1 = (L_diag[:, None]) * W\n",
    "        prod2 = (U_diag[:, None]) * W\n",
    "    else:\n",
    "        raise ValueError(\"side must be 'left' or 'right'\")\n",
    "\n",
    "    lower = np.minimum(prod1, prod2)\n",
    "    upper = np.maximum(prod1, prod2)\n",
    "    return lower, upper\n",
    "\n",
    "def multiply_known_W(W, lower, upper, side='right'):\n",
    "    if side == 'right':\n",
    "        lower_new = lower @ W\n",
    "        upper_new = upper @ W\n",
    "    elif side == 'left':\n",
    "        lower_new = W @ lower\n",
    "        upper_new = W @ upper\n",
    "    else:\n",
    "        raise ValueError(\"side must be 'left' or 'right'\")\n",
    "    lower, upper = np.minimum(lower_new, upper_new), np.maximum(lower_new,upper_new)\n",
    "    return lower- upper\n",
    "\n",
    "B = np.array([0, 0, 1])\n",
    "\n",
    "T = 1000\n",
    "dt = 0.01\n",
    "\n",
    "xs = np.zeros((3, T))\n",
    "x = xs[:,0] + 2\n",
    "print(x.size)\n",
    "for i in range(T):\n",
    "    x += dt * drift(x)\n",
    "    xs[:,i] = x\n",
    "\n",
    "print(xs[:,-1])\n",
    "\n",
    "plt.plot(xs[2,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5fb6256",
   "metadata": {},
   "outputs": [],
   "source": [
    "xlow = np.array([-0.5, -0.5, -0.5])\n",
    "xhigh = np.array([0.5, 0.5, 0.5])\n",
    "\n",
    "W1 = np.random.randn(3, 16)\n",
    "W2 = np.random.randn(16)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab574e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = np.array([[1.0, 2.0], [-1.0, 0.5]])\n",
    "W2 = np.array([[0.5, -1.0], [2.0,  1.0]])\n",
    "W3 = np.array([[1.0, 0.0], [0.0, 1.0]])\n",
    "\n",
    "L1 = np.array([0.5, -1.0])\n",
    "U1 = np.array([1.5,  0.0])\n",
    "\n",
    "L2 = np.array([-0.5, 0.2])\n",
    "U2 = np.array([1.0,  1.0])\n",
    "\n",
    "'''\n",
    "# TODOs\n",
    "1. Compute intermediate interval bounds on the z_k\n",
    "2. Compute the bounds on the J_i, L_i <= J_i <= U_i\n",
    "3. Compute the bounds then on the product defining Du(x)\n",
    "'''\n",
    "\n",
    "def linear_interval(W, l, u):\n",
    "    W_plus = np.maximum(W, 0)\n",
    "    W_minus = W - W_plus\n",
    "    \n",
    "    lower = W_plus @ l + W_minus @ u\n",
    "    upper = W_plus @ u + W_minus @ l\n",
    "    return lower, upper\n",
    "\n",
    "def get_hidden_preactivation_bounds(x_lower, x_over, W1, W2):\n",
    "    bounds = []\n",
    "    l1, u1 = linear_interval(W1, x_lower, x_over)\n",
    "\n",
    "    l_inter, u_inter = smooth_relu(l1), smooth_relu(u1)\n",
    "    l2, u2 = linear_interval(W2, l1, u1)\n",
    "    return l1, u1\n",
    "\n",
    "def get_diagonal_bounds(l, u):\n",
    "    return smooth_deriv(l), smooth_deriv(u)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af125e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class NN_IBP(nn.Module):\n",
    "    def __init__(self, input_dim=3, hidden_dims=[16, 16], output_dim=1, activation='softplus', trainable_NCM=False):\n",
    "        super().__init__()\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        self.activations = nn.ModuleList()\n",
    "        if trainable_NCM:\n",
    "            self.P = torch.zeros((3, 3), requires_grad = True)\n",
    "        else:\n",
    "            self.P = torch.zeros((3, 3), requires_grad = False)\n",
    "\n",
    "        dims = [input_dim] + hidden_dims\n",
    "        for i in range(len(hidden_dims)):\n",
    "            self.hidden_layers.append(nn.Linear(dims[i], dims[i+1]))\n",
    "            if activation == 'softplus':\n",
    "                self.activations.append(nn.Softplus())\n",
    "            elif activation == 'relu':\n",
    "                self.activations.append(nn.ReLU())\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported activation: {activation}\")\n",
    "\n",
    "        self.output_layer = nn.Linear(hidden_dims[-1], output_dim)\n",
    "\n",
    "    def constant_NCM(self):\n",
    "        return self.P @ torch.transpose(self.P, 0, 1) + torch.eye(self.P.shape[-1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer, act in zip(self.hidden_layers, self.activations):\n",
    "            x = act(layer(x))\n",
    "        return self.output_layer(x)\n",
    "\n",
    "    def get_hidden_pre_activation_bounds(self, x_lower, x_upper):\n",
    "        \"\"\"\n",
    "        Compute pre-activation interval bounds for each hidden layer.\n",
    "\n",
    "        Args:\n",
    "            x_lower: Tensor of shape (B, input_dim)\n",
    "            x_upper: Tensor of shape (B, input_dim)\n",
    "\n",
    "        Returns:\n",
    "            List of tuples (l_i, u_i): pre-activation bounds at each hidden layer\n",
    "        \"\"\"\n",
    "        def linear_interval(layer: nn.Linear, l, u):\n",
    "            W = layer.weight\n",
    "            b = layer.bias\n",
    "            W_pos = torch.clamp(W, min=0)\n",
    "            W_neg = torch.clamp(W, max=0)\n",
    "\n",
    "            lower = torch.matmul(l, W_pos.T) + torch.matmul(u, W_neg.T) + b\n",
    "            upper = torch.matmul(u, W_pos.T) + torch.matmul(l, W_neg.T) + b\n",
    "            return lower, upper\n",
    "\n",
    "        def activation_interval(act, l, u):\n",
    "            return act(l), act(u)\n",
    "\n",
    "        bounds = []\n",
    "\n",
    "        l, u = x_lower, x_upper\n",
    "        for layer, act in zip(self.hidden_layers, self.activations):\n",
    "            l_pre, u_pre = linear_interval(layer, l, u)\n",
    "            bounds.append((l_pre, u_pre))\n",
    "            l, u = activation_interval(act, l_pre, u_pre)\n",
    "\n",
    "        return bounds\n",
    "    \n",
    "    def activation_derivative(self, x):\n",
    "        # return 1/(1 + torch.exp(-x))\n",
    "        alpha = 0.3\n",
    "        return alpha + (1 - alpha) * torch.sigmoid(x)\n",
    "\n",
    "    def get_diagonal_bounds_from_intermediate(self, pre_act_bounds):\n",
    "        diagonal_bounds = []\n",
    "        for i in range(len(pre_act_bounds)):\n",
    "            lower_diag_bound = self.activation_derivative(pre_act_bounds[i][0])\n",
    "            upper_diag_bound = self.activation_derivative(pre_act_bounds[i][1])\n",
    "            diagonal_bounds.append((lower_diag_bound, upper_diag_bound))\n",
    "        return diagonal_bounds\n",
    "\n",
    "    def get_diag_bounds(self, l, u):\n",
    "        pre_act_bounds = self.get_hidden_pre_activation_bounds(l, u)\n",
    "        return self.get_diagonal_bounds_from_intermediate(pre_act_bounds)\n",
    "    \n",
    "    def left_multiply_by_diag(self, J_lower, J_upper, P_lower, P_upper):\n",
    "        \"\"\"\n",
    "        Batched version: Compute bounds on J @ P, where J is diagonal\n",
    "        with diag entries in [J_lower, J_upper], and P is an interval matrix.\n",
    "\n",
    "        Args:\n",
    "            J_lower, J_upper: (b, n)\n",
    "            P_lower, P_upper: (b, n, m)\n",
    "\n",
    "        Returns:\n",
    "            (lower, upper): (b, n, m)\n",
    "        \"\"\"\n",
    "        # Expand diagonal bounds to match P's shape for broadcasting\n",
    "        J_lower_exp = J_lower.unsqueeze(-1)  # (b, n, 1)\n",
    "        J_upper_exp = J_upper.unsqueeze(-1)  # (b, n, 1)\n",
    "\n",
    "        # Four possible products\n",
    "        ll = J_lower_exp * P_lower\n",
    "        lu = J_lower_exp * P_upper\n",
    "        ul = J_upper_exp * P_lower\n",
    "        uu = J_upper_exp * P_upper\n",
    "\n",
    "        # Elementwise min/max over the four cases\n",
    "        lower = torch.minimum(torch.minimum(ll, lu), torch.minimum(ul, uu))\n",
    "        upper = torch.maximum(torch.maximum(ll, lu), torch.maximum(ul, uu))\n",
    "\n",
    "        return lower, upper\n",
    "\n",
    "\n",
    "    def left_multiply_by_constant_matrix(self, P_lower, P_upper, W):\n",
    "        \"\"\"\n",
    "        Batched version: Compute bounds on W @ P, given bounds on P.\n",
    "\n",
    "        Args:\n",
    "            P_lower, P_upper: (b, n, m)\n",
    "            W: (k, n) constant matrix\n",
    "\n",
    "        Returns:\n",
    "            (lower, upper): (b, k, m)\n",
    "        \"\"\"\n",
    "        # Use batch matrix multiply: (b, k, n) @ (b, n, m)\n",
    "        W_exp = W.unsqueeze(0).expand(P_lower.shape[0], -1, -1)  # (b, k, n)\n",
    "\n",
    "        X1 = torch.bmm(W_exp, P_lower)\n",
    "        X2 = torch.bmm(W_exp, P_upper)\n",
    "\n",
    "        lower = torch.minimum(X1, X2)\n",
    "        upper = torch.maximum(X1, X2)\n",
    "\n",
    "        return lower, upper\n",
    "\n",
    "    def compute_full_product_bound(self, diag_bounds_list, elision_matrix):\n",
    "        \"\"\"\n",
    "        Computes interval bounds on the product:\n",
    "        W_out · J_N · W_{N-1} · J_{N-1} · ... · J_2 · W_1\n",
    "\n",
    "        Batched version.\n",
    "\n",
    "        Args:\n",
    "            diag_bounds_list: list of (J_lower_i, J_upper_i), each of shape (b, n_i)\n",
    "                            length = N-1\n",
    "            elision_matrix: optional matrix to multiply with W_out at the end\n",
    "\n",
    "        Returns:\n",
    "            (M_lower, M_upper): tensors of shape (b, k, m) — bounds on the final matrix product\n",
    "        \"\"\"\n",
    "        batch_size = diag_bounds_list[0][0].shape[0]\n",
    "        assert len(diag_bounds_list) == len(self.hidden_layers), \\\n",
    "            f\"Expected {len(self.hidden_layers)} diag bounds but got {len(diag_bounds_list)}\"\n",
    "\n",
    "        # Step 1: Start from the rightmost matrix: W_1\n",
    "        W1 = self.hidden_layers[0].weight.clone()  # (n1, m1)\n",
    "        P_lower = W1.unsqueeze(0).expand(batch_size, -1, -1).clone()\n",
    "        P_upper = W1.unsqueeze(0).expand(batch_size, -1, -1).clone()\n",
    "\n",
    "        # Step 2: Loop through J_2, W_2, ..., J_N\n",
    "        for i in range(1, len(self.hidden_layers)):\n",
    "            # Multiply on the left with diag(J_i)\n",
    "            J_lower, J_upper = diag_bounds_list[i - 1]  # (b, n_i)\n",
    "            P_lower, P_upper = self.left_multiply_by_diag(J_lower, J_upper, P_lower, P_upper)\n",
    "\n",
    "            # Multiply on the left with constant W_i\n",
    "            W_i = self.hidden_layers[i].weight  # (n_{i+1}, n_i)\n",
    "            P_lower, P_upper = self.left_multiply_by_constant_matrix(P_lower, P_upper, W_i)\n",
    "\n",
    "        # Step 3: Final left multiplication with J_N\n",
    "        J_lower, J_upper = diag_bounds_list[-1]\n",
    "        P_lower, P_upper = self.left_multiply_by_diag(J_lower, J_upper, P_lower, P_upper)\n",
    "\n",
    "        # Step 4: Multiply by W_out\n",
    "        W_out = self.output_layer.weight.clone()  # (k, n_last)\n",
    "        if elision_matrix is not None:\n",
    "            W_out = elision_matrix @ W_out\n",
    "        final_lower, final_upper = self.left_multiply_by_constant_matrix(P_lower, P_upper, W_out)\n",
    "\n",
    "        return final_lower, final_upper\n",
    "\n",
    "    \n",
    "    def compute_Du_bounds(self, l, u, elision_matrix = None):\n",
    "        diag_bounds = self.get_diag_bounds(l, u)\n",
    "        return self.compute_full_product_bound(diag_bounds, elision_matrix)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "ad786f4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n0. Incorporate elision for constant contraction metric case -- DONE\\n1. Compute the interval bounds on the given vector field -- DONE\\n2. Computer upper bound on Metzlerization -- DONE\\n3. Check spectral abscissa -- DONE\\n4. ???\\n4\\n5. Partitioning\\n6. Neural contraction metrics\\n7. Training?\\n'"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "0. Incorporate elision for constant contraction metric case -- DONE\n",
    "1. Compute the interval bounds on the given vector field -- DONE\n",
    "2. Computer upper bound on Metzlerization -- DONE\n",
    "3. Check spectral abscissa -- DONE\n",
    "4. ???\n",
    "4\n",
    "5. Partitioning\n",
    "6. Neural contraction metrics\n",
    "7. Training?\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "463f8ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    # Example from Manchester and Slotine\n",
    "    return torch.tensor([-x[0] + x[2], \n",
    "    x[0]**2 - x[1] - 2*x[0]*x[2] + x[2], \n",
    "    -x[1]])\n",
    "\n",
    "def Df(x):\n",
    "    \"\"\"\n",
    "    Jacobian of vector field for batched input.\n",
    "    \n",
    "    Args:\n",
    "        x: Tensor of shape (batch_size, 3) or (3,)\n",
    "    Returns:\n",
    "        Jacobian: Tensor of shape (batch_size, 3, 3)\n",
    "    \"\"\"\n",
    "    # Ensure batch dimension\n",
    "    if x.ndim == 1:\n",
    "        x = x.unsqueeze(0)\n",
    "\n",
    "    batch_size = x.shape[0]\n",
    "    J = torch.zeros(batch_size, 3, 3, dtype=x.dtype, device=x.device)\n",
    "\n",
    "    # Fill in the Jacobian\n",
    "    J[:, 0, 0] = -1\n",
    "    J[:, 0, 1] = 0\n",
    "    J[:, 0, 2] = 1\n",
    "\n",
    "    J[:, 1, 0] = 2*x[:, 0] - 2*x[:, 2]\n",
    "    J[:, 1, 1] = -1\n",
    "    J[:, 1, 2] = -2*x[:, 0] + 1\n",
    "\n",
    "    J[:, 2, 0] = 0\n",
    "    J[:, 2, 1] = -1\n",
    "    J[:, 2, 2] = 0\n",
    "\n",
    "    return J\n",
    "\n",
    "def jac_bounds_old(l, u):\n",
    "    lower = torch.tensor([[-1, 0, 1],\n",
    "                         [2*l[0] - 2*u[2], -1, -2*u[0] + 1],\n",
    "                         [0, -1, 0]])\n",
    "    upper = torch.tensor([[-1, 0, 1],\n",
    "                         [2*u[0] - 2*l[2], -1, -2*l[0] + 1],\n",
    "                         [0, -1, 0]])\n",
    "    return lower, upper\n",
    "\n",
    "def jac_bounds(l, u):\n",
    "    # l, u: (B, 3) or (3,) if unbatched\n",
    "    # Ensure batching\n",
    "    if l.ndim == 1:\n",
    "        l = l.unsqueeze(0)\n",
    "        u = u.unsqueeze(0)\n",
    "    \n",
    "    B = l.shape[0]\n",
    "    device = l.device\n",
    "    dtype = l.dtype\n",
    "    \n",
    "    lower = torch.zeros((B, 3, 3), device=device, dtype=dtype)\n",
    "    upper = torch.zeros((B, 3, 3), device=device, dtype=dtype)\n",
    "    \n",
    "    # Fill in constant entries\n",
    "    lower[:, 0, 0] = -1\n",
    "    lower[:, 0, 2] = 1\n",
    "    lower[:, 1, 1] = -1\n",
    "    lower[:, 2, 1] = -1\n",
    "    \n",
    "    upper[:, 0, 0] = -1\n",
    "    upper[:, 0, 2] = 1\n",
    "    upper[:, 1, 1] = -1\n",
    "    upper[:, 2, 1] = -1\n",
    "    \n",
    "    # Fill in l/u dependent entries\n",
    "    lower[:, 1, 0] = 2*l[:, 0] - 2*u[:, 2]\n",
    "    lower[:, 1, 2] = -2*u[:, 0] + 1\n",
    "    \n",
    "    upper[:, 1, 0] = 2*u[:, 0] - 2*l[:, 2]\n",
    "    upper[:, 1, 2] = -2*l[:, 0] + 1\n",
    "    \n",
    "    return (lower, upper)\n",
    "\n",
    "\n",
    "B = torch.tensor([[0.], [0.], [1.]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "b8198398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4277, 0.4228, 0.4182, 0.4159, 0.4280, 0.4223, 0.4184, 0.4157, 0.4278,\n",
      "        0.4225, 0.4184, 0.4162, 0.4271, 0.4228, 0.4186, 0.4157, 0.4208, 0.4176,\n",
      "        0.4150, 0.4165, 0.4204, 0.4170, 0.4151, 0.4160, 0.4205, 0.4175, 0.4153,\n",
      "        0.4159, 0.4208, 0.4173, 0.4151, 0.4164, 0.4159, 0.4151, 0.4173, 0.4207,\n",
      "        0.4162, 0.4152, 0.4176, 0.4208, 0.4161, 0.4152, 0.4175, 0.4207, 0.4163,\n",
      "        0.4153, 0.4173, 0.4208, 0.4160, 0.4185, 0.4224, 0.4277, 0.4161, 0.4187,\n",
      "        0.4223, 0.4270, 0.4161, 0.4187, 0.4227, 0.4277, 0.4159, 0.4187, 0.4228,\n",
      "        0.4280])\n"
     ]
    }
   ],
   "source": [
    "def max_eig_over_hyperrectangles(J_func, l, u, P, num_samples=100):\n",
    "    \"\"\"\n",
    "    Compute the maximum eigenvalue of P @ J(x) + J(x).T @ P\n",
    "    over random samples inside each hyperrectangle.\n",
    "\n",
    "    Args:\n",
    "        J_func: callable, takes (..., dim) tensor and returns (..., n, n) Jacobians.\n",
    "        bounds: tensor of shape (batch, dim, 2) giving [min, max] for each dim.\n",
    "        P: (n, n) symmetric matrix.\n",
    "        num_samples: number of samples per hyperrectangle.\n",
    "\n",
    "    Returns:\n",
    "        max_eigs: tensor of shape (batch,) with max eigenvalue per hyperrectangle.\n",
    "    \"\"\"\n",
    "    batch_size, dim = l.shape\n",
    "    device = l.device\n",
    "    n = P.shape[0]\n",
    "\n",
    "    # Sample uniformly in each hyperrectangle\n",
    "    rand = torch.rand(batch_size, num_samples, dim, device=device)\n",
    "    samples = l[:, None, :] + rand * (u - l)[:, None, :]  # (batch, num_samples, dim)\n",
    "\n",
    "    # Flatten samples for batch processing\n",
    "    flat_samples = samples.reshape(-1, dim)  # (batch * num_samples, dim)\n",
    "\n",
    "    # Evaluate J(x) for all samples\n",
    "    J_vals = J_func(flat_samples)  # (batch*num_samples, n, n)\n",
    "\n",
    "    # Compute PJ + J^T P\n",
    "    PJ = torch.matmul(P, J_vals)  # (batch*num_samples, n, n)\n",
    "    JTP = torch.matmul(J_vals.transpose(-1, -2), P)\n",
    "    M = PJ + JTP  # (batch*num_samples, n, n)\n",
    "\n",
    "    # Compute largest eigenvalue for each matrix\n",
    "    eigvals = torch.linalg.eigvalsh(M)  # (..., n)\n",
    "    max_eigs_per_sample = eigvals[..., -1]  # (batch*num_samples,)\n",
    "\n",
    "    # Reshape back to (batch, num_samples) and take max over samples\n",
    "    max_eigs = max_eigs_per_sample.view(batch_size, num_samples).max(dim=1).values\n",
    "\n",
    "    return max_eigs\n",
    "\n",
    "P = torch.eye(3)\n",
    "print(max_eig_over_hyperrectangles(Df, xunder, xover, P))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "df48b455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "NN_IBP(\n",
      "  (hidden_layers): ModuleList(\n",
      "    (0): Linear(in_features=3, out_features=16, bias=True)\n",
      "    (1): Linear(in_features=16, out_features=16, bias=True)\n",
      "  )\n",
      "  (activations): ModuleList(\n",
      "    (0-1): 2 x Softplus(beta=1.0, threshold=20.0)\n",
      "  )\n",
      "  (output_layer): Linear(in_features=16, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nWant bound on P @ Df + Df^T P + P B Du + (P B Du)^T\\n'"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_metzler_upper_bound(jac_bounds, controller_bounds):\n",
    "    jac_lower, jac_upper = jac_bounds       # (B, n, n)\n",
    "    Du_lower, Du_upper = controller_bounds  # (B, n, n)\n",
    "    \n",
    "    # Transpose only the last two dims for batching\n",
    "    jac_lower_T = jac_lower.transpose(-1, -2)\n",
    "    jac_upper_T = jac_upper.transpose(-1, -2)\n",
    "    Du_lower_T = Du_lower.transpose(-1, -2)\n",
    "    Du_upper_T = Du_upper.transpose(-1, -2)\n",
    "    \n",
    "    # Compute LMI lower and upper bounds (batch-wise)\n",
    "    lmi_lower = 0.5 * (jac_lower + jac_lower_T + Du_lower + Du_lower_T)\n",
    "    lmi_upper = 0.5 * (jac_upper + jac_upper_T + Du_upper + Du_upper_T)\n",
    "    \n",
    "    # Elementwise maximum for the Metzler bound\n",
    "    mat_abs = torch.maximum(lmi_upper, -lmi_lower)\n",
    "    \n",
    "    # Extract diagonals in batched way\n",
    "    diag_mat_abs = torch.diagonal(mat_abs, dim1=-2, dim2=-1)\n",
    "    diag_lmi_upper = torch.diagonal(lmi_upper, dim1=-2, dim2=-1)\n",
    "    \n",
    "    # Zero out diagonal of mat_abs, then replace with lmi_upper diagonal\n",
    "    result = mat_abs - torch.diag_embed(diag_mat_abs) + torch.diag_embed(diag_lmi_upper)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def compute_metzler_upper_bound_new(jac_eigenbounds, controller_bounds):\n",
    "    #jac_lower, jac_upper = jac_bounds       # (B, n, n)\n",
    "    Du_lower, Du_upper = controller_bounds  # (B, n, n)\n",
    "    \n",
    "    # Transpose only the last two dims for batching\n",
    "    #jac_lower_T = jac_lower.transpose(-1, -2)\n",
    "    #jac_upper_T = jac_upper.transpose(-1, -2)\n",
    "    Du_lower_T = Du_lower.transpose(-1, -2)\n",
    "    Du_upper_T = Du_upper.transpose(-1, -2)\n",
    "    \n",
    "    # Compute LMI lower and upper bounds (batch-wise)\n",
    "    lmi_lower = (Du_lower + Du_lower_T)\n",
    "    lmi_upper = (Du_upper + Du_upper_T)\n",
    "    \n",
    "    # Elementwise maximum for the Metzler bound\n",
    "    mat_abs = torch.maximum(lmi_upper, -lmi_lower)\n",
    "    \n",
    "    # Extract diagonals in batched way\n",
    "    diag_mat_abs = torch.diagonal(mat_abs, dim1=-2, dim2=-1)\n",
    "    diag_lmi_upper = torch.diagonal(lmi_upper, dim1=-2, dim2=-1)\n",
    "    \n",
    "    # Zero out diagonal of mat_abs, then replace with lmi_upper diagonal\n",
    "    result = mat_abs - torch.diag_embed(diag_mat_abs) + torch.diag_embed(diag_lmi_upper) + jac_eigenbounds[:, None, None] * torch.eye(Du_upper.shape[-1])\n",
    "    print(result[0])\n",
    "    return result\n",
    "\n",
    "\n",
    "def max_eig_metzler(M, num_iter=200, tol=1e-8):\n",
    "    \"\"\"\n",
    "    Compute max eigenvalue of batched Metzler matrices M using power iteration.\n",
    "\n",
    "    Args:\n",
    "        M: (b, n, n) batched Metzler matrices\n",
    "        num_iter: max iterations\n",
    "        tol: convergence tolerance\n",
    "\n",
    "    Returns:\n",
    "        lambda_max: (b,) largest eigenvalues per batch\n",
    "    \"\"\"\n",
    "    bshape = M.shape[:-2]\n",
    "    n = M.shape[-1]\n",
    "    v = torch.ones(*bshape, n, dtype=M.dtype, device=M.device)\n",
    "    v = v / v.sum(dim=-1, keepdim=True)\n",
    "\n",
    "    for _ in range(num_iter):\n",
    "        v_next = torch.matmul(M, v.unsqueeze(-1)).squeeze(-1)\n",
    "        norm = v_next.sum(dim=-1, keepdim=True)\n",
    "        v_next = v_next / (norm + 1e-12)\n",
    "        v_next = torch.clamp(v_next, min=1e-8)\n",
    "        \n",
    "        # Convergence check (L1 difference)\n",
    "        if torch.max(torch.abs(v_next - v)) < tol:\n",
    "            break\n",
    "        v = v_next\n",
    "\n",
    "    # Rayleigh quotient estimate\n",
    "    numerator = torch.sum(v * torch.matmul(M, v.unsqueeze(-1)).squeeze(-1), dim=-1)\n",
    "    denominator = torch.sum(v * v, dim=-1)\n",
    "    lambda_max = numerator / (denominator + 1e-12)\n",
    "    return lambda_max\n",
    "\n",
    "def smooth_relu(x, delta=1.0):\n",
    "    # x: tensor\n",
    "    # delta: smoothing interval width > 0\n",
    "\n",
    "    zero = torch.zeros_like(x)\n",
    "    # mask regions\n",
    "    mask_neg = (x <= 0)\n",
    "    mask_smooth = (x > 0) & (x < delta)\n",
    "    mask_linear = (x >= delta)\n",
    "\n",
    "    a = -2.0 / (delta ** 3)\n",
    "    b = 3.0 / (delta ** 2)\n",
    "\n",
    "    # Compute smooth cubic part\n",
    "    smooth_part = a * x**3 + b * x**2\n",
    "\n",
    "    return torch.where(\n",
    "        mask_neg, zero,\n",
    "        torch.where(\n",
    "            mask_smooth, smooth_part,\n",
    "            x  # linear region\n",
    "        )\n",
    "    )\n",
    "\n",
    "def max_eig_metzler_shifted(M, num_iter=50, tol=1e-4):\n",
    "    \"\"\"\n",
    "    Compute the max eigenvalue of a batched symmetric Metzler matrix M using power iteration,\n",
    "    with spectral shift to ensure nonnegativity for stable convergence.\n",
    "\n",
    "    Args:\n",
    "        M: (b, n, n) batched symmetric Metzler matrices\n",
    "        num_iter: maximum power iteration steps\n",
    "        tol: convergence tolerance\n",
    "\n",
    "    Returns:\n",
    "        lambda_max: (b,) largest eigenvalue estimates per batch\n",
    "    \"\"\"\n",
    "    bshape = M.shape[:-2]\n",
    "    n = M.shape[-1]\n",
    "    device = M.device\n",
    "    dtype = M.dtype\n",
    "\n",
    "    # Compute the shift scalar: at least -min diagonal entry, plus small margin\n",
    "    # Since M is Metzler, off-diagonal >= 0 but diagonals can be negative\n",
    "    min_diag, _ = torch.min(torch.diagonal(M, dim1=-2, dim2=-1), dim=-1, keepdim=True)  # (b,1)\n",
    "    shift = -min_diag + 1e-3  # small positive margin\n",
    "    shift = shift.unsqueeze(-1)  # shape (b,1,1) for broadcasting\n",
    "\n",
    "    # Shift the matrix: M_shifted = M + shift * I\n",
    "    I = torch.eye(n, device=device, dtype=dtype).unsqueeze(0).expand(*bshape, n, n)\n",
    "    M_shifted = M + shift * I\n",
    "\n",
    "    # Initialize positive starting vector (batch)\n",
    "    v = torch.ones(*bshape, n, device=device, dtype=dtype)\n",
    "    v = v / v.sum(dim=-1, keepdim=True)\n",
    "\n",
    "    for _ in range(num_iter):\n",
    "        v_next = torch.matmul(M_shifted, v.unsqueeze(-1)).squeeze(-1)\n",
    "        norm = v_next.sum(dim=-1, keepdim=True)\n",
    "        v_next = v_next / (norm + 1e-12)\n",
    "        v_next = torch.clamp(v_next, min=1e-8)\n",
    "        \n",
    "        if torch.max(torch.abs(v_next - v)) < tol:\n",
    "            break\n",
    "        v = v_next\n",
    "\n",
    "    # Rayleigh quotient for shifted matrix\n",
    "    numerator = torch.sum(v * torch.matmul(M_shifted, v.unsqueeze(-1)).squeeze(-1), dim=-1)\n",
    "    denominator = torch.sum(v * v, dim=-1)\n",
    "    lambda_max_shifted = numerator / (denominator + 1e-12)\n",
    "\n",
    "    # Subtract shift to get original max eigenvalue\n",
    "    lambda_max = lambda_max_shifted - shift.squeeze(-1).squeeze(-1)\n",
    "\n",
    "    return lambda_max\n",
    "\n",
    "def partition_hyperrectangle(xunder, xover, n_parts):\n",
    "    \"\"\"\n",
    "    Partition a hyperrectangle [xunder, xover] into n_parts equally sized sub-rectangles.\n",
    "\n",
    "    Args:\n",
    "        xunder (torch.Tensor): Lower corner, shape (d,)\n",
    "        xover (torch.Tensor): Upper corner, shape (d,)\n",
    "        n_parts (int): Number of partitions (must be a^d for some integer a)\n",
    "\n",
    "    Returns:\n",
    "        sub_xunder (torch.Tensor): Lower corners of sub-rectangles, shape (n_parts, d)\n",
    "        sub_xover (torch.Tensor): Upper corners of sub-rectangles, shape (n_parts, d)\n",
    "    \"\"\"\n",
    "    dim = xunder.shape[0]\n",
    "    # Determine number of splits along each axis\n",
    "    splits_per_dim = round(n_parts ** (1.0 / dim))\n",
    "    assert splits_per_dim ** dim == n_parts, \\\n",
    "        \"n_parts must be a perfect power of the number of dimensions\"\n",
    "\n",
    "    # Create equally spaced points along each dimension\n",
    "    edges = [\n",
    "        torch.linspace(xunder[i], xover[i], splits_per_dim + 1)\n",
    "        for i in range(dim)\n",
    "    ]\n",
    "\n",
    "    # Lower and upper bounds for each small rectangle\n",
    "    lower_grid = torch.stack(torch.meshgrid(*[e[:-1] for e in edges], indexing='ij'), dim=-1)\n",
    "    upper_grid = torch.stack(torch.meshgrid(*[e[1:] for e in edges], indexing='ij'), dim=-1)\n",
    "\n",
    "    sub_xunder = lower_grid.reshape(-1, dim)\n",
    "    sub_xover = upper_grid.reshape(-1, dim)\n",
    "\n",
    "    return sub_xunder, sub_xover\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "xunder = torch.tensor([0.0, 0.0, 0.0])\n",
    "xover = torch.tensor([1.0, 2.0, 3.0])\n",
    "n_parts = 64  # 4^3\n",
    "\n",
    "sub_xunder, sub_xover = partition_hyperrectangle(xunder, xover, n_parts)\n",
    "\n",
    "print(sub_xunder.shape)  # (64, 3)\n",
    "print(sub_xover.shape)   # (64, 3)\n",
    "\n",
    "\n",
    "model = NN_IBP()\n",
    "print(model)\n",
    "\n",
    "xunder = -0.5*torch.ones(3)\n",
    "xover = -xunder\n",
    "\n",
    "diag_bounds = model.get_diag_bounds(xunder, xover)\n",
    "#print(diag_bounds)\n",
    "\n",
    "Du_lower, Du_upper = model.compute_Du_bounds(xunder, xover, elision_matrix=B)\n",
    "jac_lower, jac_upper = jac_bounds(xunder, xover)\n",
    "\n",
    "'''\n",
    "Want bound on P @ Df + Df^T P + P B Du + (P B Du)^T\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "1c0926fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1000/20000] Loss: 0.0069 \n",
      "Epoch [2000/20000] Loss: 0.0064 \n",
      "Epoch [3000/20000] Loss: 0.0071 \n",
      "Epoch [4000/20000] Loss: 0.0049 \n",
      "Epoch [5000/20000] Loss: 0.0044 \n",
      "Epoch [6000/20000] Loss: 0.0029 \n",
      "Epoch [7000/20000] Loss: 0.0025 \n",
      "Epoch [8000/20000] Loss: 0.0020 \n",
      "Epoch [9000/20000] Loss: 0.0015 \n",
      "Epoch [10000/20000] Loss: 0.0012 \n",
      "Epoch [11000/20000] Loss: 0.0010 \n",
      "Epoch [12000/20000] Loss: 0.0008 \n",
      "Epoch [13000/20000] Loss: 0.0007 \n",
      "Epoch [14000/20000] Loss: 0.0006 \n",
      "Epoch [15000/20000] Loss: 0.0004 \n",
      "At epoch  15876  loss has hit 0, valid closed-loop contracting controller\n",
      "Metzler upper bound: tensor([[-1.0000e+00,  1.6667e-01,  2.5340e-03],\n",
      "        [ 1.6667e-01, -1.0000e+00,  9.2516e-01],\n",
      "        [ 2.5340e-03,  9.2516e-01, -3.3309e+02]], grad_fn=<SelectBackward0>)\n",
      "tensor([[-3.3309e+02, -1.1654e+00, -8.3203e-01],\n",
      "        [-3.3219e+02, -1.3327e+00, -6.6591e-01],\n",
      "        [-3.2230e+02, -1.4997e+00, -4.9956e-01],\n",
      "        [-3.1364e+02, -1.6666e+00, -3.3306e-01],\n",
      "        [-3.0224e+02, -1.8333e+00, -1.6659e-01],\n",
      "        [-3.0833e+02, -2.0000e+00,  1.4779e-05],\n",
      "        [-3.3369e+02, -1.1654e+00, -8.3204e-01],\n",
      "        [-3.3267e+02, -1.3327e+00, -6.6591e-01],\n",
      "        [-3.2262e+02, -1.4997e+00, -4.9955e-01],\n",
      "        [-3.1395e+02, -1.6666e+00, -3.3305e-01],\n",
      "        [-3.0243e+02, -1.8333e+00, -1.6659e-01],\n",
      "        [-3.0868e+02, -2.0000e+00,  1.5241e-05],\n",
      "        [-3.3430e+02, -1.1654e+00, -8.3204e-01],\n",
      "        [-3.3315e+02, -1.3327e+00, -6.6590e-01],\n",
      "        [-3.2295e+02, -1.4997e+00, -4.9954e-01],\n",
      "        [-3.1425e+02, -1.6666e+00, -3.3304e-01],\n",
      "        [-3.0264e+02, -1.8333e+00, -1.6659e-01],\n",
      "        [-3.0902e+02, -2.0000e+00,  1.5159e-05],\n",
      "        [-3.3490e+02, -1.1654e+00, -8.3205e-01],\n",
      "        [-3.3363e+02, -1.3327e+00, -6.6590e-01],\n",
      "        [-3.2327e+02, -1.4997e+00, -4.9953e-01],\n",
      "        [-3.1455e+02, -1.6666e+00, -3.3303e-01],\n",
      "        [-3.0284e+02, -1.8333e+00, -1.6659e-01],\n",
      "        [-3.0934e+02, -2.0000e+00,  1.5110e-05],\n",
      "        [-3.3550e+02, -1.1654e+00, -8.3206e-01],\n",
      "        [-3.3410e+02, -1.3327e+00, -6.6589e-01],\n",
      "        [-3.2359e+02, -1.4997e+00, -4.9952e-01],\n",
      "        [-3.1486e+02, -1.6666e+00, -3.3302e-01],\n",
      "        [-3.0304e+02, -1.8333e+00, -1.6659e-01],\n",
      "        [-3.0960e+02, -2.0000e+00,  1.4146e-05],\n",
      "        [-3.3610e+02, -1.1654e+00, -8.3207e-01],\n",
      "        [-3.3457e+02, -1.3327e+00, -6.6589e-01],\n",
      "        [-3.2391e+02, -1.4997e+00, -4.9951e-01],\n",
      "        [-3.1516e+02, -1.6666e+00, -3.3301e-01],\n",
      "        [-3.0325e+02, -1.8333e+00, -1.6658e-01],\n",
      "        [-3.0987e+02, -2.0000e+00,  1.2844e-05],\n",
      "        [-3.3310e+02, -1.3325e+00, -6.6580e-01],\n",
      "        [-3.3220e+02, -1.1663e+00, -8.3289e-01],\n",
      "        [-3.2229e+02, -1.3332e+00, -6.6645e-01],\n",
      "        [-3.1369e+02, -1.5000e+00, -4.9989e-01],\n",
      "        [-3.0224e+02, -1.6666e+00, -3.3328e-01],\n",
      "        [-3.0841e+02, -1.8332e+00, -1.6655e-01],\n",
      "        [-3.3370e+02, -1.3325e+00, -6.6580e-01],\n",
      "        [-3.3268e+02, -1.1663e+00, -8.3289e-01],\n",
      "        [-3.2261e+02, -1.3332e+00, -6.6644e-01],\n",
      "        [-3.1399e+02, -1.5000e+00, -4.9989e-01],\n",
      "        [-3.0244e+02, -1.6666e+00, -3.3328e-01],\n",
      "        [-3.0873e+02, -1.8332e+00, -1.6655e-01],\n",
      "        [-3.3430e+02, -1.3325e+00, -6.6581e-01],\n",
      "        [-3.3316e+02, -1.1663e+00, -8.3288e-01],\n",
      "        [-3.2293e+02, -1.3332e+00, -6.6644e-01],\n",
      "        [-3.1430e+02, -1.5000e+00, -4.9988e-01],\n",
      "        [-3.0264e+02, -1.6666e+00, -3.3328e-01],\n",
      "        [-3.0899e+02, -1.8332e+00, -1.6655e-01],\n",
      "        [-3.3490e+02, -1.3325e+00, -6.6581e-01],\n",
      "        [-3.3364e+02, -1.1663e+00, -8.3288e-01],\n",
      "        [-3.2325e+02, -1.3332e+00, -6.6643e-01],\n",
      "        [-3.1461e+02, -1.5000e+00, -4.9988e-01],\n",
      "        [-3.0284e+02, -1.6666e+00, -3.3328e-01],\n",
      "        [-3.0925e+02, -1.8332e+00, -1.6656e-01],\n",
      "        [-3.3550e+02, -1.3325e+00, -6.6582e-01],\n",
      "        [-3.3411e+02, -1.1663e+00, -8.3288e-01],\n",
      "        [-3.2357e+02, -1.3332e+00, -6.6642e-01],\n",
      "        [-3.1491e+02, -1.5000e+00, -4.9987e-01],\n",
      "        [-3.0305e+02, -1.6666e+00, -3.3328e-01],\n",
      "        [-3.0952e+02, -1.8332e+00, -1.6656e-01],\n",
      "        [-3.3609e+02, -1.3325e+00, -6.6582e-01],\n",
      "        [-3.3457e+02, -1.1663e+00, -8.3287e-01],\n",
      "        [-3.2389e+02, -1.3332e+00, -6.6642e-01],\n",
      "        [-3.1522e+02, -1.5000e+00, -4.9986e-01],\n",
      "        [-3.0326e+02, -1.6666e+00, -3.3329e-01],\n",
      "        [-3.0979e+02, -1.8332e+00, -1.6656e-01],\n",
      "        [-3.3311e+02, -1.4995e+00, -4.9947e-01],\n",
      "        [-3.3220e+02, -1.3332e+00, -6.6645e-01],\n",
      "        [-3.2227e+02, -1.1667e+00, -8.3326e-01],\n",
      "        [-3.1373e+02, -1.3333e+00, -6.6645e-01],\n",
      "        [-3.0223e+02, -1.4999e+00, -4.9980e-01],\n",
      "        [-3.0839e+02, -1.6664e+00, -3.3302e-01],\n",
      "        [-3.3371e+02, -1.4995e+00, -4.9947e-01],\n",
      "        [-3.3268e+02, -1.3332e+00, -6.6645e-01],\n",
      "        [-3.2259e+02, -1.1667e+00, -8.3327e-01],\n",
      "        [-3.1404e+02, -1.3333e+00, -6.6645e-01],\n",
      "        [-3.0244e+02, -1.4999e+00, -4.9980e-01],\n",
      "        [-3.0864e+02, -1.6664e+00, -3.3302e-01],\n",
      "        [-3.3430e+02, -1.4995e+00, -4.9947e-01],\n",
      "        [-3.3316e+02, -1.3332e+00, -6.6645e-01],\n",
      "        [-3.2291e+02, -1.1667e+00, -8.3326e-01],\n",
      "        [-3.1435e+02, -1.3333e+00, -6.6645e-01],\n",
      "        [-3.0264e+02, -1.4999e+00, -4.9980e-01],\n",
      "        [-3.0890e+02, -1.6664e+00, -3.3302e-01],\n",
      "        [-3.3490e+02, -1.4995e+00, -4.9948e-01],\n",
      "        [-3.3364e+02, -1.3332e+00, -6.6645e-01],\n",
      "        [-3.2323e+02, -1.1667e+00, -8.3325e-01],\n",
      "        [-3.1465e+02, -1.3333e+00, -6.6645e-01],\n",
      "        [-3.0285e+02, -1.4999e+00, -4.9980e-01],\n",
      "        [-3.0917e+02, -1.6664e+00, -3.3303e-01],\n",
      "        [-3.3549e+02, -1.4995e+00, -4.9948e-01],\n",
      "        [-3.3411e+02, -1.3332e+00, -6.6645e-01],\n",
      "        [-3.2355e+02, -1.1667e+00, -8.3325e-01],\n",
      "        [-3.1496e+02, -1.3333e+00, -6.6645e-01],\n",
      "        [-3.0306e+02, -1.4999e+00, -4.9980e-01],\n",
      "        [-3.0943e+02, -1.6664e+00, -3.3303e-01],\n",
      "        [-3.3608e+02, -1.4995e+00, -4.9949e-01],\n",
      "        [-3.3457e+02, -1.3332e+00, -6.6644e-01],\n",
      "        [-3.2387e+02, -1.1667e+00, -8.3325e-01],\n",
      "        [-3.1527e+02, -1.3333e+00, -6.6645e-01],\n",
      "        [-3.0327e+02, -1.4999e+00, -4.9981e-01],\n",
      "        [-3.0971e+02, -1.6664e+00, -3.3304e-01],\n",
      "        [-3.3312e+02, -1.6664e+00, -3.3306e-01],\n",
      "        [-3.3221e+02, -1.5000e+00, -4.9993e-01],\n",
      "        [-3.2224e+02, -1.3333e+00, -6.6647e-01],\n",
      "        [-3.1377e+02, -1.1665e+00, -8.3287e-01],\n",
      "        [-3.0223e+02, -1.3330e+00, -6.6621e-01],\n",
      "        [-3.0830e+02, -1.4995e+00, -4.9940e-01],\n",
      "        [-3.3371e+02, -1.6664e+00, -3.3306e-01],\n",
      "        [-3.3268e+02, -1.5000e+00, -4.9993e-01],\n",
      "        [-3.2257e+02, -1.3333e+00, -6.6647e-01],\n",
      "        [-3.1408e+02, -1.1665e+00, -8.3287e-01],\n",
      "        [-3.0243e+02, -1.3330e+00, -6.6621e-01],\n",
      "        [-3.0856e+02, -1.4995e+00, -4.9939e-01],\n",
      "        [-3.3431e+02, -1.6664e+00, -3.3306e-01],\n",
      "        [-3.3316e+02, -1.5000e+00, -4.9993e-01],\n",
      "        [-3.2289e+02, -1.3333e+00, -6.6646e-01],\n",
      "        [-3.1438e+02, -1.1665e+00, -8.3287e-01],\n",
      "        [-3.0264e+02, -1.3330e+00, -6.6622e-01],\n",
      "        [-3.0882e+02, -1.4995e+00, -4.9939e-01],\n",
      "        [-3.3490e+02, -1.6664e+00, -3.3306e-01],\n",
      "        [-3.3363e+02, -1.5000e+00, -4.9993e-01],\n",
      "        [-3.2321e+02, -1.3333e+00, -6.6646e-01],\n",
      "        [-3.1469e+02, -1.1665e+00, -8.3287e-01],\n",
      "        [-3.0285e+02, -1.3330e+00, -6.6622e-01],\n",
      "        [-3.0909e+02, -1.4995e+00, -4.9940e-01],\n",
      "        [-3.3549e+02, -1.6664e+00, -3.3307e-01],\n",
      "        [-3.3410e+02, -1.5000e+00, -4.9993e-01],\n",
      "        [-3.2352e+02, -1.3333e+00, -6.6646e-01],\n",
      "        [-3.1500e+02, -1.1665e+00, -8.3287e-01],\n",
      "        [-3.0306e+02, -1.3330e+00, -6.6622e-01],\n",
      "        [-3.0936e+02, -1.4995e+00, -4.9940e-01],\n",
      "        [-3.3608e+02, -1.6664e+00, -3.3307e-01],\n",
      "        [-3.3456e+02, -1.5000e+00, -4.9993e-01],\n",
      "        [-3.2384e+02, -1.3333e+00, -6.6646e-01],\n",
      "        [-3.1531e+02, -1.1665e+00, -8.3286e-01],\n",
      "        [-3.0327e+02, -1.3330e+00, -6.6623e-01],\n",
      "        [-3.0963e+02, -1.4995e+00, -4.9941e-01],\n",
      "        [-3.3313e+02, -1.8332e+00, -1.6656e-01],\n",
      "        [-3.3220e+02, -1.6666e+00, -3.3325e-01],\n",
      "        [-3.2221e+02, -1.4998e+00, -4.9958e-01],\n",
      "        [-3.1380e+02, -1.3329e+00, -6.6587e-01],\n",
      "        [-3.0222e+02, -1.1660e+00, -8.3253e-01],\n",
      "        [-3.0822e+02, -1.3325e+00, -6.6567e-01],\n",
      "        [-3.3372e+02, -1.8332e+00, -1.6656e-01],\n",
      "        [-3.3268e+02, -1.6666e+00, -3.3325e-01],\n",
      "        [-3.2253e+02, -1.4998e+00, -4.9957e-01],\n",
      "        [-3.1411e+02, -1.3329e+00, -6.6587e-01],\n",
      "        [-3.0243e+02, -1.1660e+00, -8.3253e-01],\n",
      "        [-3.0848e+02, -1.3325e+00, -6.6567e-01],\n",
      "        [-3.3431e+02, -1.8332e+00, -1.6656e-01],\n",
      "        [-3.3315e+02, -1.6666e+00, -3.3325e-01],\n",
      "        [-3.2285e+02, -1.4998e+00, -4.9957e-01],\n",
      "        [-3.1442e+02, -1.3329e+00, -6.6587e-01],\n",
      "        [-3.0263e+02, -1.1661e+00, -8.3253e-01],\n",
      "        [-3.0875e+02, -1.3325e+00, -6.6567e-01],\n",
      "        [-3.3489e+02, -1.8332e+00, -1.6656e-01],\n",
      "        [-3.3362e+02, -1.6666e+00, -3.3325e-01],\n",
      "        [-3.2317e+02, -1.4998e+00, -4.9957e-01],\n",
      "        [-3.1473e+02, -1.3329e+00, -6.6587e-01],\n",
      "        [-3.0285e+02, -1.1661e+00, -8.3254e-01],\n",
      "        [-3.0901e+02, -1.3325e+00, -6.6567e-01],\n",
      "        [-3.3548e+02, -1.8332e+00, -1.6657e-01],\n",
      "        [-3.3409e+02, -1.6666e+00, -3.3325e-01],\n",
      "        [-3.2349e+02, -1.4998e+00, -4.9957e-01],\n",
      "        [-3.1504e+02, -1.3330e+00, -6.6587e-01],\n",
      "        [-3.0306e+02, -1.1661e+00, -8.3255e-01],\n",
      "        [-3.0928e+02, -1.3325e+00, -6.6568e-01],\n",
      "        [-3.3607e+02, -1.8332e+00, -1.6657e-01],\n",
      "        [-3.3455e+02, -1.6666e+00, -3.3325e-01],\n",
      "        [-3.2380e+02, -1.4998e+00, -4.9957e-01],\n",
      "        [-3.1535e+02, -1.3330e+00, -6.6587e-01],\n",
      "        [-3.0327e+02, -1.1661e+00, -8.3256e-01],\n",
      "        [-3.0956e+02, -1.3325e+00, -6.6569e-01],\n",
      "        [-3.3314e+02, -2.0000e+00,  1.5108e-05],\n",
      "        [-3.3219e+02, -1.8332e+00, -1.6643e-01],\n",
      "        [-3.2218e+02, -1.6662e+00, -3.3259e-01],\n",
      "        [-3.1382e+02, -1.4993e+00, -4.9878e-01],\n",
      "        [-3.0221e+02, -1.3323e+00, -6.6542e-01],\n",
      "        [-3.0815e+02, -1.1654e+00, -8.3184e-01],\n",
      "        [-3.3372e+02, -2.0000e+00,  1.5009e-05],\n",
      "        [-3.3267e+02, -1.8332e+00, -1.6643e-01],\n",
      "        [-3.2249e+02, -1.6662e+00, -3.3259e-01],\n",
      "        [-3.1413e+02, -1.4993e+00, -4.9877e-01],\n",
      "        [-3.0242e+02, -1.3324e+00, -6.6542e-01],\n",
      "        [-3.0841e+02, -1.1654e+00, -8.3183e-01],\n",
      "        [-3.3431e+02, -2.0000e+00,  1.4675e-05],\n",
      "        [-3.3314e+02, -1.8332e+00, -1.6643e-01],\n",
      "        [-3.2281e+02, -1.6662e+00, -3.3259e-01],\n",
      "        [-3.1444e+02, -1.4993e+00, -4.9877e-01],\n",
      "        [-3.0263e+02, -1.3324e+00, -6.6542e-01],\n",
      "        [-3.0868e+02, -1.1654e+00, -8.3184e-01],\n",
      "        [-3.3489e+02, -2.0000e+00,  1.4432e-05],\n",
      "        [-3.3361e+02, -1.8332e+00, -1.6643e-01],\n",
      "        [-3.2313e+02, -1.6662e+00, -3.3259e-01],\n",
      "        [-3.1475e+02, -1.4993e+00, -4.9877e-01],\n",
      "        [-3.0284e+02, -1.3324e+00, -6.6543e-01],\n",
      "        [-3.0894e+02, -1.1654e+00, -8.3184e-01],\n",
      "        [-3.3547e+02, -2.0000e+00,  1.3574e-05],\n",
      "        [-3.3407e+02, -1.8332e+00, -1.6642e-01],\n",
      "        [-3.2344e+02, -1.6662e+00, -3.3259e-01],\n",
      "        [-3.1506e+02, -1.4993e+00, -4.9877e-01],\n",
      "        [-3.0305e+02, -1.3324e+00, -6.6544e-01],\n",
      "        [-3.0922e+02, -1.1654e+00, -8.3185e-01],\n",
      "        [-3.3602e+02, -2.0000e+00,  1.2894e-05],\n",
      "        [-3.3453e+02, -1.8332e+00, -1.6642e-01],\n",
      "        [-3.2376e+02, -1.6662e+00, -3.3259e-01],\n",
      "        [-3.1538e+02, -1.4993e+00, -4.9877e-01],\n",
      "        [-3.0327e+02, -1.3324e+00, -6.6545e-01],\n",
      "        [-3.0949e+02, -1.1654e+00, -8.3186e-01]],\n",
      "       grad_fn=<LinalgEighBackward0>)\n",
      "contraction metric tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "eps = 1e-8\n",
    "learning_rate = 1e-2\n",
    "weight_decay = 1e-4  # L2 regularization\n",
    "num_epochs = 20000\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "xunder = -0.5*torch.ones(3)\n",
    "eye = torch.eye(3).to(device=device)\n",
    "xover = 0.5*torch.ones(3)\n",
    "\n",
    "xunder, xover = partition_hyperrectangle(xunder, xover, 6**3)\n",
    "\n",
    "# --- Send model to device ---\n",
    "model = NN_IBP(hidden_dims=[32,32], trainable_NCM=False)\n",
    "model = model.to(device)\n",
    "\n",
    "# --- Loss and optimizer ---\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# --- Training loop ---\n",
    "for epoch in range(num_epochs):\n",
    "    # ---- Training ----\n",
    "    model.train()\n",
    "    # Check this function for bugs\n",
    "    NCM = model.constant_NCM()\n",
    "    Du_bounds = model.compute_Du_bounds(xunder, xover, elision_matrix=NCM @ B)\n",
    "\n",
    "    #print(Du_bounds[0].shape)\n",
    "    Df_bounds = jac_bounds(xunder, xover)\n",
    "    #print(Df_bounds[0].shape)\n",
    "    # eigenbounds = max_eig_over_hyperrectangles(Df, xunder, xover, NCM)\n",
    "    B_Mzr = compute_metzler_upper_bound(Df_bounds, Du_bounds)\n",
    "    # B_Mzr = compute_metzler_upper_bound_new(eigenbounds, Du_bounds)\n",
    "    # improve conditioning\n",
    "    B_Mzr = B_Mzr + eps * torch.eye(B_Mzr.shape[-1], device=B_Mzr.device).unsqueeze(0)\n",
    "    # try:\n",
    "    #     eigs = torch.linalg.eigvalsh(B_Mzr)\n",
    "    # except:\n",
    "    #     print('poor conditioning')\n",
    "    #     print(B_Mzr[0])\n",
    "        \n",
    "    #     break\n",
    "    # max_eig = eigs.amax(dim=tuple(range(1, eigs.ndim)))\n",
    "    max_eig = max_eig_metzler_shifted(B_Mzr)\n",
    "    # max_eig = B_Mzr.sum(dim=-1)\n",
    "    # print(max_eig)\n",
    "    loss = torch.sum(torch.relu(max_eig))\n",
    "    if torch.isnan(loss):\n",
    "        print('NaN detected in loss')\n",
    "        break\n",
    "\n",
    "    if loss <= 1e-7:\n",
    "        print('At epoch ', epoch+1, ' loss has hit 0, valid closed-loop contracting controller')\n",
    "        print('Metzler upper bound:', B_Mzr[0])\n",
    "        print(torch.linalg.eigvalsh(B_Mzr))\n",
    "        break\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None and torch.isnan(param.grad).any():\n",
    "            print(f\"NaN detected in gradients of {name}\")\n",
    "\n",
    "    if epoch % 1000 == 999:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "              f\"Loss: {loss:.4f} \")\n",
    "        \n",
    "\n",
    "print('contraction metric', model.constant_NCM())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "29dd6bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.0923e-05, grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "xunder = -0.5*torch.ones(3)\n",
    "xover = 0.5*torch.ones(3)\n",
    "\n",
    "xunder, xover = partition_hyperrectangle(xunder, xover, 4**3)\n",
    "\n",
    "Du_bounds = model.compute_Du_bounds(xunder, xover, elision_matrix=B)\n",
    "Df_bounds = jac_bounds(xunder, xover)\n",
    "\n",
    "B_Mzr = compute_metzler_upper_bound(Df_bounds, Du_bounds)\n",
    "loss = torch.relu(torch.max(torch.linalg.eigvalsh(B_Mzr)))\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c31a6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo:\n",
    "# 1. Handle arbitrary constant P by adjusting the method for computing bounds on Df\n",
    "# 2. Get 1 nice example working (pendulum maybe)\n",
    "# 3. Write code for general contraction metrics M(x) - perhaps another class for NCM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
