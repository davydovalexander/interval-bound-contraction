{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "fb8c1b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "[0.01235143 0.01243659 0.02712459]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x140d2a7e0>]"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cvxpy as cp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def drift(x):\n",
    "    return np.array([-x[0] + x[2], \n",
    "    x[0]**2 - x[1] - 2*x[0]*x[2] + x[2], \n",
    "    -x[1]])\n",
    "\n",
    "def NCM(x):\n",
    "    return np.eye(3)\n",
    "\n",
    "def smooth_relu(x):\n",
    "    return np.log(1 + np.exp(x))\n",
    "\n",
    "def multiply_diagonal(W, L_diag, U_diag, side='right'):\n",
    "    \"\"\"\n",
    "    Multiply matrix W with diagonal interval matrix [L_diag, U_diag].\n",
    "    \n",
    "    If side = 'right', returns bounds for W * J\n",
    "    If side = 'left',  returns bounds for J * W\n",
    "    \"\"\"\n",
    "    L_diag = np.asarray(L_diag).reshape(-1)\n",
    "    U_diag = np.asarray(U_diag).reshape(-1)\n",
    "\n",
    "    if side == 'right':\n",
    "        prod1 = W * L_diag  # broadcasting\n",
    "        prod2 = W * U_diag\n",
    "    elif side == 'left':\n",
    "        prod1 = (L_diag[:, None]) * W\n",
    "        prod2 = (U_diag[:, None]) * W\n",
    "    else:\n",
    "        raise ValueError(\"side must be 'left' or 'right'\")\n",
    "\n",
    "    lower = np.minimum(prod1, prod2)\n",
    "    upper = np.maximum(prod1, prod2)\n",
    "    return lower, upper\n",
    "\n",
    "def multiply_known_W(W, lower, upper, side='right'):\n",
    "    if side == 'right':\n",
    "        lower_new = lower @ W\n",
    "        upper_new = upper @ W\n",
    "    elif side == 'left':\n",
    "        lower_new = W @ lower\n",
    "        upper_new = W @ upper\n",
    "    else:\n",
    "        raise ValueError(\"side must be 'left' or 'right'\")\n",
    "    lower, upper = np.minimum(lower_new, upper_new), np.maximum(lower_new,upper_new)\n",
    "    return lower- upper\n",
    "\n",
    "B = np.array([0, 0, 1])\n",
    "\n",
    "T = 1000\n",
    "dt = 0.01\n",
    "\n",
    "xs = np.zeros((3, T))\n",
    "x = xs[:,0] + 2.2\n",
    "print(x.size)\n",
    "for i in range(T):\n",
    "    x += dt * drift(x)\n",
    "    xs[:,i] = x\n",
    "\n",
    "print(xs[:,-1])\n",
    "\n",
    "plt.plot(xs[2,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5fb6256",
   "metadata": {},
   "outputs": [],
   "source": [
    "xlow = np.array([-0.5, -0.5, -0.5])\n",
    "xhigh = np.array([0.5, 0.5, 0.5])\n",
    "\n",
    "W1 = np.random.randn(3, 16)\n",
    "W2 = np.random.randn(16)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab574e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = np.array([[1.0, 2.0], [-1.0, 0.5]])\n",
    "W2 = np.array([[0.5, -1.0], [2.0,  1.0]])\n",
    "W3 = np.array([[1.0, 0.0], [0.0, 1.0]])\n",
    "\n",
    "L1 = np.array([0.5, -1.0])\n",
    "U1 = np.array([1.5,  0.0])\n",
    "\n",
    "L2 = np.array([-0.5, 0.2])\n",
    "U2 = np.array([1.0,  1.0])\n",
    "\n",
    "'''\n",
    "# TODOs\n",
    "1. Compute intermediate interval bounds on the z_k\n",
    "2. Compute the bounds on the J_i, L_i <= J_i <= U_i\n",
    "3. Compute the bounds then on the product defining Du(x)\n",
    "'''\n",
    "\n",
    "def linear_interval(W, l, u):\n",
    "    W_plus = np.maximum(W, 0)\n",
    "    W_minus = W - W_plus\n",
    "    \n",
    "    lower = W_plus @ l + W_minus @ u\n",
    "    upper = W_plus @ u + W_minus @ l\n",
    "    return lower, upper\n",
    "\n",
    "def get_hidden_preactivation_bounds(x_lower, x_over, W1, W2):\n",
    "    bounds = []\n",
    "    l1, u1 = linear_interval(W1, x_lower, x_over)\n",
    "\n",
    "    l_inter, u_inter = smooth_relu(l1), smooth_relu(u1)\n",
    "    l2, u2 = linear_interval(W2, l1, u1)\n",
    "    return l1, u1\n",
    "\n",
    "def get_diagonal_bounds(l, u):\n",
    "    return smooth_deriv(l), smooth_deriv(u)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "af125e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class NN_IBP(nn.Module):\n",
    "    def __init__(self, input_dim=3, hidden_dims=[16, 16], output_dim=1, activation='softplus', trainable_NCM=False):\n",
    "        super().__init__()\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        self.activations = nn.ModuleList()\n",
    "        if trainable_NCM:\n",
    "            self.P = nn.Linear(input_dim, input_dim, bias = False)\n",
    "        else:\n",
    "            self.P = nn.Linear(input_dim, input_dim, bias = False)\n",
    "            self.P.weight.requires_grad = False\n",
    "\n",
    "        dims = [input_dim] + hidden_dims\n",
    "        for i in range(len(hidden_dims)):\n",
    "            self.hidden_layers.append(nn.Linear(dims[i], dims[i+1]))\n",
    "            if activation == 'softplus':\n",
    "                self.activations.append(nn.Softplus())\n",
    "            elif activation == 'relu':\n",
    "                self.activations.append(nn.ReLU())\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported activation: {activation}\")\n",
    "\n",
    "        self.output_layer = nn.Linear(hidden_dims[-1], output_dim)\n",
    "\n",
    "    def constant_NCM(self):\n",
    "        return self.P.weight @ torch.transpose(self.P.weight, 0, 1) + torch.eye(self.P.weight.shape[-1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer, act in zip(self.hidden_layers, self.activations):\n",
    "            x = act(layer(x))\n",
    "        return self.output_layer(x)\n",
    "\n",
    "    def get_hidden_pre_activation_bounds(self, x_lower, x_upper):\n",
    "        \"\"\"\n",
    "        Compute pre-activation interval bounds for each hidden layer.\n",
    "\n",
    "        Args:\n",
    "            x_lower: Tensor of shape (B, input_dim)\n",
    "            x_upper: Tensor of shape (B, input_dim)\n",
    "\n",
    "        Returns:\n",
    "            List of tuples (l_i, u_i): pre-activation bounds at each hidden layer\n",
    "        \"\"\"\n",
    "        def linear_interval(layer: nn.Linear, l, u):\n",
    "            W = layer.weight\n",
    "            b = layer.bias\n",
    "            W_pos = torch.clamp(W, min=0)\n",
    "            W_neg = torch.clamp(W, max=0)\n",
    "\n",
    "            lower = torch.matmul(l, W_pos.T) + torch.matmul(u, W_neg.T) + b\n",
    "            upper = torch.matmul(u, W_pos.T) + torch.matmul(l, W_neg.T) + b\n",
    "            return lower, upper\n",
    "\n",
    "        def activation_interval(act, l, u):\n",
    "            return act(l), act(u)\n",
    "\n",
    "        bounds = []\n",
    "\n",
    "        l, u = x_lower, x_upper\n",
    "        for layer, act in zip(self.hidden_layers, self.activations):\n",
    "            l_pre, u_pre = linear_interval(layer, l, u)\n",
    "            bounds.append((l_pre, u_pre))\n",
    "            l, u = activation_interval(act, l_pre, u_pre)\n",
    "\n",
    "        return bounds\n",
    "    \n",
    "    def activation_derivative(self, x):\n",
    "        # return 1/(1 + torch.exp(-x))\n",
    "        #alpha = 0.3\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "    def get_diagonal_bounds_from_intermediate(self, pre_act_bounds):\n",
    "        diagonal_bounds = []\n",
    "        for i in range(len(pre_act_bounds)):\n",
    "            lower_diag_bound = self.activation_derivative(pre_act_bounds[i][0])\n",
    "            upper_diag_bound = self.activation_derivative(pre_act_bounds[i][1])\n",
    "            diagonal_bounds.append((lower_diag_bound, upper_diag_bound))\n",
    "        return diagonal_bounds\n",
    "\n",
    "    def get_diag_bounds(self, l, u):\n",
    "        pre_act_bounds = self.get_hidden_pre_activation_bounds(l, u)\n",
    "        return self.get_diagonal_bounds_from_intermediate(pre_act_bounds)\n",
    "    \n",
    "    def left_multiply_by_diag(self, J_lower, J_upper, P_lower, P_upper):\n",
    "        \"\"\"\n",
    "        Batched version: Compute bounds on J @ P, where J is diagonal\n",
    "        with diag entries in [J_lower, J_upper], and P is an interval matrix.\n",
    "\n",
    "        Args:\n",
    "            J_lower, J_upper: (b, n)\n",
    "            P_lower, P_upper: (b, n, m)\n",
    "\n",
    "        Returns:\n",
    "            (lower, upper): (b, n, m)\n",
    "        \"\"\"\n",
    "        # Expand diagonal bounds to match P's shape for broadcasting\n",
    "        J_lower_exp = J_lower.unsqueeze(-1)  # (b, n, 1)\n",
    "        J_upper_exp = J_upper.unsqueeze(-1)  # (b, n, 1)\n",
    "\n",
    "        # Four possible products\n",
    "        ll = J_lower_exp * P_lower\n",
    "        lu = J_lower_exp * P_upper\n",
    "        ul = J_upper_exp * P_lower\n",
    "        uu = J_upper_exp * P_upper\n",
    "\n",
    "        # Elementwise min/max over the four cases\n",
    "        lower = torch.minimum(torch.minimum(ll, lu), torch.minimum(ul, uu))\n",
    "        upper = torch.maximum(torch.maximum(ll, lu), torch.maximum(ul, uu))\n",
    "\n",
    "        return lower, upper\n",
    "\n",
    "\n",
    "    def left_multiply_by_constant_matrix(self, P_lower, P_upper, W):\n",
    "        \"\"\"\n",
    "        Batched version: Compute bounds on W @ P, given bounds on P.\n",
    "\n",
    "        Args:\n",
    "            P_lower, P_upper: (b, n, m)\n",
    "            W: (k, n) constant matrix\n",
    "\n",
    "        Returns:\n",
    "            (lower, upper): (b, k, m)\n",
    "        \"\"\"\n",
    "        # Use batch matrix multiply: (b, k, n) @ (b, n, m)\n",
    "        W_exp = W.unsqueeze(0).expand(P_lower.shape[0], -1, -1)  # (b, k, n)\n",
    "\n",
    "        X1 = torch.bmm(W_exp, P_lower)\n",
    "        X2 = torch.bmm(W_exp, P_upper)\n",
    "\n",
    "        lower = torch.minimum(X1, X2)\n",
    "        upper = torch.maximum(X1, X2)\n",
    "\n",
    "        return lower, upper\n",
    "\n",
    "    def compute_full_product_bound(self, diag_bounds_list, elision_matrix):\n",
    "        \"\"\"\n",
    "        Computes interval bounds on the product:\n",
    "        W_out · J_N · W_{N-1} · J_{N-1} · ... · J_2 · W_1\n",
    "\n",
    "        Batched version.\n",
    "\n",
    "        Args:\n",
    "            diag_bounds_list: list of (J_lower_i, J_upper_i), each of shape (b, n_i)\n",
    "                            length = N-1\n",
    "            elision_matrix: optional matrix to multiply with W_out at the end\n",
    "\n",
    "        Returns:\n",
    "            (M_lower, M_upper): tensors of shape (b, k, m) — bounds on the final matrix product\n",
    "        \"\"\"\n",
    "        batch_size = diag_bounds_list[0][0].shape[0]\n",
    "        assert len(diag_bounds_list) == len(self.hidden_layers), \\\n",
    "            f\"Expected {len(self.hidden_layers)} diag bounds but got {len(diag_bounds_list)}\"\n",
    "\n",
    "        # Step 1: Start from the rightmost matrix: W_1\n",
    "        W1 = self.hidden_layers[0].weight.clone()  # (n1, m1)\n",
    "        P_lower = W1.unsqueeze(0).expand(batch_size, -1, -1).clone()\n",
    "        P_upper = W1.unsqueeze(0).expand(batch_size, -1, -1).clone()\n",
    "\n",
    "        # Step 2: Loop through J_2, W_2, ..., J_N\n",
    "        for i in range(1, len(self.hidden_layers)):\n",
    "            # Multiply on the left with diag(J_i)\n",
    "            J_lower, J_upper = diag_bounds_list[i - 1]  # (b, n_i)\n",
    "            P_lower, P_upper = self.left_multiply_by_diag(J_lower, J_upper, P_lower, P_upper)\n",
    "\n",
    "            # Multiply on the left with constant W_i\n",
    "            W_i = self.hidden_layers[i].weight  # (n_{i+1}, n_i)\n",
    "            P_lower, P_upper = self.left_multiply_by_constant_matrix(P_lower, P_upper, W_i)\n",
    "\n",
    "        # Step 3: Final left multiplication with J_N\n",
    "        J_lower, J_upper = diag_bounds_list[-1]\n",
    "        P_lower, P_upper = self.left_multiply_by_diag(J_lower, J_upper, P_lower, P_upper)\n",
    "\n",
    "        # Step 4: Multiply by W_out\n",
    "        W_out = self.output_layer.weight.clone()  # (k, n_last)\n",
    "        if elision_matrix is not None:\n",
    "            W_out = elision_matrix @ W_out\n",
    "        final_lower, final_upper = self.left_multiply_by_constant_matrix(P_lower, P_upper, W_out)\n",
    "\n",
    "        return final_lower, final_upper\n",
    "\n",
    "    \n",
    "    def compute_Du_bounds(self, l, u, elision_matrix = None):\n",
    "        diag_bounds = self.get_diag_bounds(l, u)\n",
    "        return self.compute_full_product_bound(diag_bounds, elision_matrix)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "ad786f4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n0. Incorporate elision for constant contraction metric case -- DONE\\n1. Compute the interval bounds on the given vector field -- DONE\\n2. Computer upper bound on Metzlerization -- DONE\\n3. Check spectral abscissa -- DONE\\n4. ???\\n4\\n5. Partitioning\\n6. Neural contraction metrics\\n7. Training?\\n'"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "0. Incorporate elision for constant contraction metric case -- DONE\n",
    "1. Compute the interval bounds on the given vector field -- DONE\n",
    "2. Computer upper bound on Metzlerization -- DONE\n",
    "3. Check spectral abscissa -- DONE\n",
    "4. ???\n",
    "4\n",
    "5. Partitioning\n",
    "6. Neural contraction metrics\n",
    "7. Training?\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "463f8ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    # Example from Manchester and Slotine\n",
    "    return torch.tensor([-x[0] + x[2], \n",
    "    x[0]**2 - x[1] - 2*x[0]*x[2] + x[2], \n",
    "    -x[1]])\n",
    "\n",
    "def Df(x):\n",
    "    \"\"\"\n",
    "    Jacobian of vector field for batched input.\n",
    "    \n",
    "    Args:\n",
    "        x: Tensor of shape (batch_size, 3) or (3,)\n",
    "    Returns:\n",
    "        Jacobian: Tensor of shape (batch_size, 3, 3)\n",
    "    \"\"\"\n",
    "    # Ensure batch dimension\n",
    "    if x.ndim == 1:\n",
    "        x = x.unsqueeze(0)\n",
    "\n",
    "    batch_size = x.shape[0]\n",
    "    J = torch.zeros(batch_size, 3, 3, dtype=x.dtype, device=x.device)\n",
    "\n",
    "    # Fill in the Jacobian\n",
    "    J[:, 0, 0] = -1\n",
    "    J[:, 0, 1] = 0\n",
    "    J[:, 0, 2] = 1\n",
    "\n",
    "    J[:, 1, 0] = 2*x[:, 0] - 2*x[:, 2]\n",
    "    J[:, 1, 1] = -1\n",
    "    J[:, 1, 2] = -2*x[:, 0] + 1\n",
    "\n",
    "    J[:, 2, 0] = 0\n",
    "    J[:, 2, 1] = -1\n",
    "    J[:, 2, 2] = 0\n",
    "\n",
    "    return J\n",
    "\n",
    "def jac_bounds_old(l, u):\n",
    "    lower = torch.tensor([[-1, 0, 1],\n",
    "                         [2*l[0] - 2*u[2], -1, -2*u[0] + 1],\n",
    "                         [0, -1, 0]])\n",
    "    upper = torch.tensor([[-1, 0, 1],\n",
    "                         [2*u[0] - 2*l[2], -1, -2*l[0] + 1],\n",
    "                         [0, -1, 0]])\n",
    "    return lower, upper\n",
    "\n",
    "def jac_bounds(l, u):\n",
    "    # l, u: (B, 3) or (3,) if unbatched\n",
    "    # Ensure batching\n",
    "    if l.ndim == 1:\n",
    "        l = l.unsqueeze(0)\n",
    "        u = u.unsqueeze(0)\n",
    "    \n",
    "    B = l.shape[0]\n",
    "    device = l.device\n",
    "    dtype = l.dtype\n",
    "    \n",
    "    lower = torch.zeros((B, 3, 3), device=device, dtype=dtype)\n",
    "    upper = torch.zeros((B, 3, 3), device=device, dtype=dtype)\n",
    "    \n",
    "    # Fill in constant entries\n",
    "    lower[:, 0, 0] = -1\n",
    "    lower[:, 0, 2] = 1\n",
    "    lower[:, 1, 1] = -1\n",
    "    lower[:, 2, 1] = -1\n",
    "    \n",
    "    upper[:, 0, 0] = -1\n",
    "    upper[:, 0, 2] = 1\n",
    "    upper[:, 1, 1] = -1\n",
    "    upper[:, 2, 1] = -1\n",
    "    \n",
    "    # Fill in l/u dependent entries\n",
    "    lower[:, 1, 0] = 2*l[:, 0] - 2*u[:, 2]\n",
    "    lower[:, 1, 2] = -2*u[:, 0] + 1\n",
    "    \n",
    "    upper[:, 1, 0] = 2*u[:, 0] - 2*l[:, 2]\n",
    "    upper[:, 1, 2] = -2*l[:, 0] + 1\n",
    "    \n",
    "    return (lower, upper)\n",
    "\n",
    "\n",
    "B = torch.tensor([[0.], [0.], [1.]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "b8198398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18.9915, 19.7152, 20.9924, 23.6692, 25.6937, 28.6966, 31.4117, 35.0204,\n",
      "        37.6233, 41.8136, 18.9816, 19.8745, 21.2642, 23.2586, 25.9117, 28.9380,\n",
      "        31.5721, 35.3096, 38.2805, 42.0662, 19.1473, 19.8348, 21.1677, 23.3665,\n",
      "        25.7702, 28.9480, 31.9658, 34.9776, 38.9365, 42.2103, 19.0894, 19.7076,\n",
      "        21.4408, 23.2042, 25.8694, 28.8064, 31.4964, 35.0458, 37.8131, 41.7284,\n",
      "        19.0152, 19.8420, 21.4039, 23.2124, 26.1238, 28.9319, 31.6131, 35.3241,\n",
      "        38.7636, 41.9893, 18.9703, 19.8283, 21.3206, 23.1217, 26.1183, 28.6994,\n",
      "        32.0397, 35.0341, 38.4560, 41.6195, 18.9438, 19.8096, 21.0024, 23.3271,\n",
      "        25.5576, 28.8191, 31.8567, 35.2829, 38.5295, 41.7398, 19.2180, 19.9440,\n",
      "        21.5484, 23.0543, 25.9967, 28.5112, 31.4755, 34.4025, 38.4819, 42.1693,\n",
      "        18.9859, 19.9325, 21.1713, 23.1720, 25.8085, 28.7514, 31.7863, 35.1947,\n",
      "        38.6921, 41.9988, 19.1669, 19.8392, 21.4781, 23.0110, 25.8557, 28.9258,\n",
      "        31.0251, 35.4509, 38.0618, 41.8600, 15.4771, 15.1959, 15.8038, 18.0784,\n",
      "        20.4662, 23.3738, 26.2993, 29.6313, 32.5097, 36.8866, 15.3087, 15.0337,\n",
      "        15.8002, 17.5197, 20.6001, 23.3226, 25.9108, 29.4500, 33.3191, 37.1398,\n",
      "        15.5482, 15.0472, 16.1285, 17.5660, 20.0783, 22.9605, 26.5551, 29.5179,\n",
      "        33.0301, 36.2798, 15.4884, 15.0689, 16.0274, 17.7943, 20.4154, 23.0527,\n",
      "        26.2822, 29.8291, 33.3136, 36.7047, 15.5447, 15.1758, 16.0791, 17.9415,\n",
      "        20.4642, 23.0306, 26.1234, 29.5903, 33.3472, 36.8061, 15.6231, 14.9812,\n",
      "        15.7871, 17.7184, 20.3839, 23.3343, 25.6209, 29.4588, 32.8410, 36.4937,\n",
      "        15.2298, 15.1046, 15.9530, 17.4940, 20.3428, 23.2916, 26.4090, 29.9007,\n",
      "        33.1656, 36.9709, 15.4326, 15.0059, 15.8169, 17.9833, 20.1092, 22.9103,\n",
      "        26.3283, 28.9142, 32.6570, 36.8178, 15.6363, 14.9728, 15.8791, 18.1135,\n",
      "        20.1736, 22.6950, 26.3637, 29.4325, 33.1359, 36.2286, 15.5200, 15.2432,\n",
      "        15.9452, 17.7431, 20.4623, 22.7948, 25.9567, 29.8012, 32.4679, 36.4217,\n",
      "        13.3443, 11.8049, 11.2085, 12.4132, 14.5233, 17.1689, 20.8028, 24.0513,\n",
      "        27.7014, 30.8703, 13.4542, 11.7261, 11.2245, 12.4745, 14.2813, 17.6293,\n",
      "        21.0179, 24.0489, 28.0513, 31.4319, 13.3702, 11.7835, 10.8456, 11.9092,\n",
      "        14.5846, 17.6748, 21.0104, 24.4198, 28.1190, 31.5906, 13.2900, 11.7514,\n",
      "        11.1206, 12.0417, 14.3624, 17.3724, 21.0709, 23.9390, 27.6847, 31.2224,\n",
      "        13.5024, 11.6094, 11.0865, 12.3342, 14.5490, 17.4563, 20.0251, 24.1240,\n",
      "        27.5029, 31.2046, 13.4140, 11.8097, 11.2483, 12.5141, 14.6749, 17.0733,\n",
      "        20.5434, 24.1559, 27.1690, 31.3139, 13.4498, 11.8173, 11.1145, 12.3393,\n",
      "        14.8183, 17.5333, 21.0047, 24.4563, 28.2182, 31.8611, 13.3793, 11.8554,\n",
      "        11.2187, 12.0840, 14.4164, 17.3241, 20.3511, 24.3470, 27.4991, 31.7616,\n",
      "        13.5039, 11.5869, 11.0934, 12.4767, 14.5979, 16.9490, 20.8899, 24.2783,\n",
      "        27.8935, 31.6491, 13.5798, 11.6426, 11.1968, 12.3907, 14.7476, 17.3320,\n",
      "        20.3466, 24.2787, 27.8766, 31.6064, 14.6620, 10.7867,  8.0799,  7.4283,\n",
      "         8.4744, 11.5007, 15.0851, 18.4984, 22.4380, 26.6615, 14.3276, 10.9444,\n",
      "         8.0942,  7.1703,  9.2267, 11.6198, 15.3410, 18.8129, 22.6036, 26.3077,\n",
      "        14.4184, 10.7761,  8.1731,  7.3537,  9.2317, 12.1924, 15.0726, 18.7947,\n",
      "        22.6810, 26.4199, 14.5679, 10.9634,  7.7871,  7.3161,  9.1850, 12.2062,\n",
      "        15.0891, 18.5414, 22.8681, 25.8757, 14.6370, 10.8891,  8.1211,  7.3682,\n",
      "         8.8926, 11.7589, 15.1457, 18.8351, 22.6479, 26.1302, 14.2494, 10.6842,\n",
      "         8.0964,  7.0662,  9.1612, 11.6103, 15.3616, 18.7264, 22.7381, 26.4569,\n",
      "        14.6069, 10.9275,  8.2096,  7.2961,  8.5841, 11.2290, 15.5025, 18.9575,\n",
      "        22.6747, 26.1224, 14.4149, 10.8061,  7.9476,  7.0498,  9.0915, 11.6185,\n",
      "        15.0285, 18.9909, 22.6331, 26.5985, 14.7168, 10.8495,  7.8742,  7.3443,\n",
      "         8.7069, 11.6299, 15.4149, 18.7591, 22.1174, 26.3131, 14.5090, 10.7640,\n",
      "         7.9724,  7.1869,  8.8560, 12.1448, 15.2183, 19.1645, 22.3712, 26.1722,\n",
      "        17.6829, 13.5470,  9.7026,  5.5631,  3.1705,  6.0755,  9.9321, 14.0992,\n",
      "        17.1643, 21.7597, 17.8072, 13.3028,  9.9775,  5.8317,  3.2892,  5.9610,\n",
      "         9.7118, 12.9872, 16.9740, 21.7480, 16.9270, 13.3193,  9.7782,  5.8240,\n",
      "         3.2688,  6.6765,  9.6256, 13.5564, 17.6925, 22.0962, 17.8212, 13.5386,\n",
      "         9.8715,  5.0092,  3.3330,  5.8825,  9.8498, 13.8654, 17.6801, 21.9665,\n",
      "        17.3479, 13.4156,  9.1141,  5.8210,  3.4185,  6.4581, 10.3548, 13.3655,\n",
      "        17.9709, 21.6309, 18.0009, 13.5732,  9.4045,  5.8017,  3.3806,  6.4273,\n",
      "        10.1067, 13.5704, 18.0380, 21.9493, 17.7247, 13.0368,  9.8055,  5.4118,\n",
      "         3.6583,  6.5127, 10.0068, 13.8301, 17.5083, 21.1912, 17.6979, 13.8431,\n",
      "         9.5210,  5.7095,  3.3571,  6.0440, 10.3163, 13.5904, 17.9231, 21.8087,\n",
      "        17.6469, 13.1899,  9.9663,  5.3501,  3.3853,  5.9294, 10.1550, 14.1779,\n",
      "        17.8149, 21.4659, 17.6693, 13.6648,  9.7147,  5.9063,  3.2700,  6.6435,\n",
      "        10.2405, 13.8618, 17.2890, 21.4291, 21.9009, 17.3261, 13.7323,  9.6483,\n",
      "         5.7158,  3.4362,  5.5004,  9.6941, 13.6118, 17.6938, 21.7556, 18.1133,\n",
      "        13.7036,  9.5929,  6.2814,  3.3766,  5.6803,  9.7431, 13.8935, 17.9583,\n",
      "        21.6275, 17.7924, 13.5547, 10.0774,  6.1962,  3.5954,  5.8843,  9.4542,\n",
      "        13.4983, 17.6623, 21.5805, 17.9647, 13.6480,  9.9144,  6.4873,  3.4499,\n",
      "         5.4045,  9.2513, 13.5661, 17.6162, 21.2497, 17.5059, 13.9220, 10.2015,\n",
      "         5.9001,  3.4857,  5.1551,  9.5029, 13.5836, 17.4327, 22.0381, 17.7078,\n",
      "        13.8560,  9.8586,  6.0385,  3.4640,  5.7512,  9.7070, 13.2036, 17.3705,\n",
      "        21.8102, 17.3887, 13.3619, 10.1431,  6.7119,  3.2245,  5.3634,  9.1575,\n",
      "        13.6477, 17.5227, 21.3097, 17.3945, 14.2153, 10.2013,  5.8393,  3.1444,\n",
      "         5.2643,  9.4523, 13.2807, 17.7003, 21.5202, 17.1471, 13.4573,  9.4869,\n",
      "         5.5991,  3.3935,  5.7815,  9.7724, 13.5175, 17.5970, 21.9846, 17.9145,\n",
      "        14.0065,  9.3845,  5.7122,  3.5232,  5.8751,  9.3820, 13.2723, 17.7659,\n",
      "        26.1140, 22.7910, 19.0906, 15.2865, 12.2237,  8.5048,  7.1717,  7.7836,\n",
      "        10.8951, 14.6078, 26.4127, 22.6234, 18.7370, 15.1716, 12.2140,  9.1568,\n",
      "         7.3302,  7.9231, 11.0923, 14.5970, 26.4942, 22.3361, 18.8660, 14.9962,\n",
      "        12.1980,  8.9738,  7.0230,  8.0846, 11.0467, 14.0619, 26.3102, 22.4016,\n",
      "        18.6993, 15.2378, 11.4077,  9.2911,  7.2732,  7.8593, 11.0083, 14.5996,\n",
      "        25.8563, 22.5987, 19.0373, 14.9423, 11.8090,  8.8420,  7.3873,  7.9495,\n",
      "        10.8107, 14.5087, 26.4963, 22.4998, 18.4128, 14.6440, 11.7778,  8.5000,\n",
      "         7.1514,  7.9683, 10.9351, 14.5930, 26.7251, 22.4065, 18.0517, 14.6311,\n",
      "        11.9786,  8.6017,  7.2587,  8.1984, 10.9810, 14.6040, 26.3441, 22.2185,\n",
      "        18.8400, 15.3749, 11.6980,  8.9258,  7.1392,  7.9674, 10.7249, 14.7521,\n",
      "        26.4613, 22.7737, 17.9969, 14.9795, 11.5324,  8.8190,  7.2605,  8.0633,\n",
      "        10.7037, 14.4267, 26.0981, 22.1588, 18.9680, 15.0793, 11.9486,  9.0089,\n",
      "         7.1847,  8.1673, 10.6929, 14.4996, 31.9070, 27.5784, 24.0991, 20.5686,\n",
      "        16.6850, 14.6856, 12.3548, 11.1023, 11.8643, 13.4978, 31.8131, 27.7697,\n",
      "        24.2647, 20.6666, 17.5101, 14.7361, 12.5604, 10.9963, 11.5611, 13.3057,\n",
      "        31.1121, 27.5594, 23.9347, 20.6571, 17.7986, 14.3289, 12.1002, 10.9924,\n",
      "        11.7262, 13.5203, 31.7165, 27.7339, 23.7528, 21.0298, 17.6188, 14.8637,\n",
      "        12.3502, 11.0423, 11.6597, 13.3693, 31.6222, 27.9868, 24.2949, 20.6904,\n",
      "        17.3969, 14.3632, 12.2055, 10.9644, 11.6085, 13.4519, 31.6416, 27.8963,\n",
      "        24.0232, 20.6330, 17.3150, 14.2407, 12.3255, 11.2583, 11.6777, 13.5785,\n",
      "        31.5083, 27.4886, 23.9437, 20.8659, 17.3965, 14.2698, 12.3147, 11.1138,\n",
      "        11.5984, 13.5750, 31.4608, 27.7390, 23.8931, 20.1691, 17.3370, 14.0463,\n",
      "        12.4721, 11.0446, 11.7164, 13.4576, 31.7053, 27.5685, 24.3569, 20.7109,\n",
      "        17.5970, 14.1757, 12.5200, 11.1423, 11.8203, 13.3667, 31.6580, 27.2122,\n",
      "        24.2617, 20.9040, 17.0714, 14.0936, 12.2340, 11.3110, 11.7875, 13.4702,\n",
      "        37.0329, 33.0102, 29.5163, 25.8854, 23.1286, 19.7561, 17.8641, 16.1232,\n",
      "        15.1200, 15.4446, 37.0663, 33.3138, 29.7460, 26.0636, 22.8202, 20.3819,\n",
      "        18.0466, 16.0029, 15.1200, 15.3936, 36.6272, 33.3065, 30.0100, 26.3980,\n",
      "        22.7978, 20.3956, 17.7123, 16.0883, 15.1415, 15.5401, 36.8394, 32.5673,\n",
      "        29.3741, 26.5488, 22.7980, 20.2402, 17.8630, 16.1064, 15.2115, 15.4323,\n",
      "        36.6252, 33.2313, 29.6659, 26.5269, 23.2145, 20.0819, 17.8068, 16.0968,\n",
      "        15.0240, 15.6945, 36.2378, 33.3639, 29.6557, 25.9452, 23.3348, 20.3176,\n",
      "        17.8329, 16.1979, 15.0757, 15.5397, 36.3209, 33.0929, 29.5290, 26.0179,\n",
      "        22.6797, 20.3363, 17.7845, 15.9050, 15.0901, 15.5214, 37.0267, 33.1062,\n",
      "        29.6393, 25.7023, 23.1156, 20.3419, 17.7534, 16.2651, 15.1151, 15.3795,\n",
      "        36.3082, 32.4493, 29.7286, 25.9316, 23.1707, 20.1421, 17.4709, 16.0137,\n",
      "        15.1951, 15.6680, 36.8934, 33.2396, 29.7244, 26.5461, 23.4175, 19.8940,\n",
      "        17.4660, 16.1649, 15.1129, 15.6618, 41.7075, 38.5411, 35.0978, 31.9360,\n",
      "        28.8930, 25.6096, 23.3460, 21.0439, 19.9001, 18.9983, 41.3196, 38.5841,\n",
      "        35.3214, 32.0520, 28.6299, 25.9992, 23.3515, 20.8519, 19.8684, 19.0815,\n",
      "        41.9604, 38.2999, 35.4125, 31.5799, 28.4688, 25.8605, 23.2172, 21.5751,\n",
      "        19.6704, 19.1649, 42.0619, 38.7149, 35.0322, 31.3229, 28.4957, 26.2342,\n",
      "        23.0862, 21.0117, 20.0922, 19.0150, 42.1978, 38.0663, 35.1927, 31.8087,\n",
      "        28.7709, 25.5257, 23.3875, 21.2123, 19.7113, 19.0499, 42.2320, 38.4307,\n",
      "        34.9408, 32.0079, 28.6763, 25.3686, 23.5349, 21.3881, 19.7706, 18.8056,\n",
      "        41.7622, 38.5334, 34.3744, 31.6802, 28.6551, 25.6596, 23.4300, 21.0668,\n",
      "        19.8712, 19.0505, 41.9582, 38.3011, 34.6471, 31.0817, 28.7258, 25.4713,\n",
      "        23.2539, 21.5324, 19.8133, 18.9986, 41.7606, 37.9164, 34.6130, 32.0585,\n",
      "        28.7909, 26.1422, 23.4988, 20.9473, 19.7641, 18.9755, 41.8520, 38.7605,\n",
      "        34.8486, 31.8645, 28.8035, 25.8896, 22.9097, 21.3199, 19.7606, 19.1079])\n"
     ]
    }
   ],
   "source": [
    "def max_eig_over_hyperrectangles(J_func, l, u, P, num_samples=100):\n",
    "    \"\"\"\n",
    "    Compute the maximum eigenvalue of P @ J(x) + J(x).T @ P\n",
    "    over random samples inside each hyperrectangle.\n",
    "\n",
    "    Args:\n",
    "        J_func: callable, takes (..., dim) tensor and returns (..., n, n) Jacobians.\n",
    "        bounds: tensor of shape (batch, dim, 2) giving [min, max] for each dim.\n",
    "        P: (n, n) symmetric matrix.\n",
    "        num_samples: number of samples per hyperrectangle.\n",
    "\n",
    "    Returns:\n",
    "        max_eigs: tensor of shape (batch,) with max eigenvalue per hyperrectangle.\n",
    "    \"\"\"\n",
    "    batch_size, dim = l.shape\n",
    "    device = l.device\n",
    "    n = P.shape[0]\n",
    "\n",
    "    # Sample uniformly in each hyperrectangle\n",
    "    rand = torch.rand(batch_size, num_samples, dim, device=device)\n",
    "    samples = l[:, None, :] + rand * (u - l)[:, None, :]  # (batch, num_samples, dim)\n",
    "\n",
    "    # Flatten samples for batch processing\n",
    "    flat_samples = samples.reshape(-1, dim)  # (batch * num_samples, dim)\n",
    "\n",
    "    # Evaluate J(x) for all samples\n",
    "    J_vals = J_func(flat_samples)  # (batch*num_samples, n, n)\n",
    "\n",
    "    # Compute PJ + J^T P\n",
    "    PJ = torch.matmul(P, J_vals)  # (batch*num_samples, n, n)\n",
    "    JTP = torch.matmul(J_vals.transpose(-1, -2), P)\n",
    "    M = PJ + JTP  # (batch*num_samples, n, n)\n",
    "\n",
    "    # Compute largest eigenvalue for each matrix\n",
    "    eigvals = torch.linalg.eigvalsh(M)  # (..., n)\n",
    "    max_eigs_per_sample = eigvals[..., -1]  # (batch*num_samples,)\n",
    "\n",
    "    # Reshape back to (batch, num_samples) and take max over samples\n",
    "    max_eigs = max_eigs_per_sample.view(batch_size, num_samples).max(dim=1).values\n",
    "\n",
    "    return max_eigs\n",
    "\n",
    "P = torch.eye(3)\n",
    "print(max_eig_over_hyperrectangles(Df, xunder, xover, P))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "df48b455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "NN_IBP(\n",
      "  (hidden_layers): ModuleList(\n",
      "    (0): Linear(in_features=3, out_features=16, bias=True)\n",
      "    (1): Linear(in_features=16, out_features=16, bias=True)\n",
      "  )\n",
      "  (activations): ModuleList(\n",
      "    (0-1): 2 x Softplus(beta=1.0, threshold=20.0)\n",
      "  )\n",
      "  (P): Linear(in_features=3, out_features=3, bias=False)\n",
      "  (output_layer): Linear(in_features=16, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nWant bound on P @ Df + Df^T P + P B Du + (P B Du)^T\\n'"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_metzler_upper_bound(jac_bounds, controller_bounds):\n",
    "    jac_lower, jac_upper = jac_bounds       # (B, n, n)\n",
    "    Du_lower, Du_upper = controller_bounds  # (B, n, n)\n",
    "    \n",
    "    # Transpose only the last two dims for batching\n",
    "    jac_lower_T = jac_lower.transpose(-1, -2)\n",
    "    jac_upper_T = jac_upper.transpose(-1, -2)\n",
    "    Du_lower_T = Du_lower.transpose(-1, -2)\n",
    "    Du_upper_T = Du_upper.transpose(-1, -2)\n",
    "    \n",
    "    # Compute LMI lower and upper bounds (batch-wise)\n",
    "    lmi_lower = 0.5 * (jac_lower + jac_lower_T + Du_lower + Du_lower_T)\n",
    "    lmi_upper = 0.5 * (jac_upper + jac_upper_T + Du_upper + Du_upper_T)\n",
    "    \n",
    "    # Elementwise maximum for the Metzler bound\n",
    "    mat_abs = torch.maximum(lmi_upper, -lmi_lower)\n",
    "    \n",
    "    # Extract diagonals in batched way\n",
    "    diag_mat_abs = torch.diagonal(mat_abs, dim1=-2, dim2=-1)\n",
    "    diag_lmi_upper = torch.diagonal(lmi_upper, dim1=-2, dim2=-1)\n",
    "    \n",
    "    # Zero out diagonal of mat_abs, then replace with lmi_upper diagonal\n",
    "    result = mat_abs - torch.diag_embed(diag_mat_abs) + torch.diag_embed(diag_lmi_upper)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def compute_metzler_upper_bound_new(jac_eigenbounds, controller_bounds):\n",
    "    #jac_lower, jac_upper = jac_bounds       # (B, n, n)\n",
    "    Du_lower, Du_upper = controller_bounds  # (B, n, n)\n",
    "    \n",
    "    # Transpose only the last two dims for batching\n",
    "    #jac_lower_T = jac_lower.transpose(-1, -2)\n",
    "    #jac_upper_T = jac_upper.transpose(-1, -2)\n",
    "    Du_lower_T = Du_lower.transpose(-1, -2)\n",
    "    Du_upper_T = Du_upper.transpose(-1, -2)\n",
    "    \n",
    "    # Compute LMI lower and upper bounds (batch-wise)\n",
    "    lmi_lower = (Du_lower + Du_lower_T)\n",
    "    lmi_upper = (Du_upper + Du_upper_T)\n",
    "    \n",
    "    # Elementwise maximum for the Metzler bound\n",
    "    mat_abs = torch.maximum(lmi_upper, -lmi_lower)\n",
    "    \n",
    "    # Extract diagonals in batched way\n",
    "    diag_mat_abs = torch.diagonal(mat_abs, dim1=-2, dim2=-1)\n",
    "    diag_lmi_upper = torch.diagonal(lmi_upper, dim1=-2, dim2=-1)\n",
    "    \n",
    "    # Zero out diagonal of mat_abs, then replace with lmi_upper diagonal\n",
    "    result = mat_abs - torch.diag_embed(diag_mat_abs) + torch.diag_embed(diag_lmi_upper) + jac_eigenbounds[:, None, None] * torch.eye(Du_upper.shape[-1])\n",
    "    print(result[0])\n",
    "    return result\n",
    "\n",
    "\n",
    "def max_eig_metzler(M, num_iter=200, tol=1e-8):\n",
    "    \"\"\"\n",
    "    Compute max eigenvalue of batched Metzler matrices M using power iteration.\n",
    "\n",
    "    Args:\n",
    "        M: (b, n, n) batched Metzler matrices\n",
    "        num_iter: max iterations\n",
    "        tol: convergence tolerance\n",
    "\n",
    "    Returns:\n",
    "        lambda_max: (b,) largest eigenvalues per batch\n",
    "    \"\"\"\n",
    "    bshape = M.shape[:-2]\n",
    "    n = M.shape[-1]\n",
    "    v = torch.ones(*bshape, n, dtype=M.dtype, device=M.device)\n",
    "    v = v / v.sum(dim=-1, keepdim=True)\n",
    "\n",
    "    for _ in range(num_iter):\n",
    "        v_next = torch.matmul(M, v.unsqueeze(-1)).squeeze(-1)\n",
    "        norm = v_next.sum(dim=-1, keepdim=True)\n",
    "        v_next = v_next / (norm + 1e-12)\n",
    "        v_next = torch.clamp(v_next, min=1e-8)\n",
    "        \n",
    "        # Convergence check (L1 difference)\n",
    "        if torch.max(torch.abs(v_next - v)) < tol:\n",
    "            break\n",
    "        v = v_next\n",
    "\n",
    "    # Rayleigh quotient estimate\n",
    "    numerator = torch.sum(v * torch.matmul(M, v.unsqueeze(-1)).squeeze(-1), dim=-1)\n",
    "    denominator = torch.sum(v * v, dim=-1)\n",
    "    lambda_max = numerator / (denominator + 1e-12)\n",
    "    return lambda_max\n",
    "\n",
    "def smooth_relu(x, delta=1.0):\n",
    "    # x: tensor\n",
    "    # delta: smoothing interval width > 0\n",
    "\n",
    "    zero = torch.zeros_like(x)\n",
    "    # mask regions\n",
    "    mask_neg = (x <= 0)\n",
    "    mask_smooth = (x > 0) & (x < delta)\n",
    "    mask_linear = (x >= delta)\n",
    "\n",
    "    a = -2.0 / (delta ** 3)\n",
    "    b = 3.0 / (delta ** 2)\n",
    "\n",
    "    # Compute smooth cubic part\n",
    "    smooth_part = a * x**3 + b * x**2\n",
    "\n",
    "    return torch.where(\n",
    "        mask_neg, zero,\n",
    "        torch.where(\n",
    "            mask_smooth, smooth_part,\n",
    "            x  # linear region\n",
    "        )\n",
    "    )\n",
    "\n",
    "def max_eig_metzler_shifted(M, num_iter=50, tol=1e-4):\n",
    "    \"\"\"\n",
    "    Compute the max eigenvalue of a batched symmetric Metzler matrix M using power iteration,\n",
    "    with spectral shift to ensure nonnegativity for stable convergence.\n",
    "\n",
    "    Args:\n",
    "        M: (b, n, n) batched symmetric Metzler matrices\n",
    "        num_iter: maximum power iteration steps\n",
    "        tol: convergence tolerance\n",
    "\n",
    "    Returns:\n",
    "        lambda_max: (b,) largest eigenvalue estimates per batch\n",
    "    \"\"\"\n",
    "    bshape = M.shape[:-2]\n",
    "    n = M.shape[-1]\n",
    "    device = M.device\n",
    "    dtype = M.dtype\n",
    "\n",
    "    # Compute the shift scalar: at least -min diagonal entry, plus small margin\n",
    "    # Since M is Metzler, off-diagonal >= 0 but diagonals can be negative\n",
    "    min_diag, _ = torch.min(torch.diagonal(M, dim1=-2, dim2=-1), dim=-1, keepdim=True)  # (b,1)\n",
    "    shift = -min_diag + 1e-3  # small positive margin\n",
    "    shift = shift.unsqueeze(-1)  # shape (b,1,1) for broadcasting\n",
    "\n",
    "    # Shift the matrix: M_shifted = M + shift * I\n",
    "    I = torch.eye(n, device=device, dtype=dtype).unsqueeze(0).expand(*bshape, n, n)\n",
    "    M_shifted = M + shift * I\n",
    "\n",
    "    # Initialize positive starting vector (batch)\n",
    "    v = torch.ones(*bshape, n, device=device, dtype=dtype)\n",
    "    v = v / v.sum(dim=-1, keepdim=True)\n",
    "\n",
    "    for _ in range(num_iter):\n",
    "        v_next = torch.matmul(M_shifted, v.unsqueeze(-1)).squeeze(-1)\n",
    "        norm = v_next.sum(dim=-1, keepdim=True)\n",
    "        v_next = v_next / (norm + 1e-12)\n",
    "        v_next = torch.clamp(v_next, min=1e-8)\n",
    "        \n",
    "        if torch.max(torch.abs(v_next - v)) < tol:\n",
    "            break\n",
    "        v = v_next\n",
    "\n",
    "    # Rayleigh quotient for shifted matrix\n",
    "    numerator = torch.sum(v * torch.matmul(M_shifted, v.unsqueeze(-1)).squeeze(-1), dim=-1)\n",
    "    denominator = torch.sum(v * v, dim=-1)\n",
    "    lambda_max_shifted = numerator / (denominator + 1e-12)\n",
    "\n",
    "    # Subtract shift to get original max eigenvalue\n",
    "    lambda_max = lambda_max_shifted - shift.squeeze(-1).squeeze(-1)\n",
    "\n",
    "    return lambda_max\n",
    "\n",
    "def partition_hyperrectangle(xunder, xover, n_parts):\n",
    "    \"\"\"\n",
    "    Partition a hyperrectangle [xunder, xover] into n_parts equally sized sub-rectangles.\n",
    "\n",
    "    Args:\n",
    "        xunder (torch.Tensor): Lower corner, shape (d,)\n",
    "        xover (torch.Tensor): Upper corner, shape (d,)\n",
    "        n_parts (int): Number of partitions (must be a^d for some integer a)\n",
    "\n",
    "    Returns:\n",
    "        sub_xunder (torch.Tensor): Lower corners of sub-rectangles, shape (n_parts, d)\n",
    "        sub_xover (torch.Tensor): Upper corners of sub-rectangles, shape (n_parts, d)\n",
    "    \"\"\"\n",
    "    dim = xunder.shape[0]\n",
    "    # Determine number of splits along each axis\n",
    "    splits_per_dim = round(n_parts ** (1.0 / dim))\n",
    "    assert splits_per_dim ** dim == n_parts, \\\n",
    "        \"n_parts must be a perfect power of the number of dimensions\"\n",
    "\n",
    "    # Create equally spaced points along each dimension\n",
    "    edges = [\n",
    "        torch.linspace(xunder[i], xover[i], splits_per_dim + 1)\n",
    "        for i in range(dim)\n",
    "    ]\n",
    "\n",
    "    # Lower and upper bounds for each small rectangle\n",
    "    lower_grid = torch.stack(torch.meshgrid(*[e[:-1] for e in edges], indexing='ij'), dim=-1)\n",
    "    upper_grid = torch.stack(torch.meshgrid(*[e[1:] for e in edges], indexing='ij'), dim=-1)\n",
    "\n",
    "    sub_xunder = lower_grid.reshape(-1, dim)\n",
    "    sub_xover = upper_grid.reshape(-1, dim)\n",
    "\n",
    "    return sub_xunder, sub_xover\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "xunder = torch.tensor([0.0, 0.0, 0.0])\n",
    "xover = torch.tensor([1.0, 2.0, 3.0])\n",
    "n_parts = 64  # 4^3\n",
    "\n",
    "sub_xunder, sub_xover = partition_hyperrectangle(xunder, xover, n_parts)\n",
    "\n",
    "print(sub_xunder.shape)  # (64, 3)\n",
    "print(sub_xover.shape)   # (64, 3)\n",
    "\n",
    "\n",
    "model = NN_IBP()\n",
    "print(model)\n",
    "\n",
    "xunder = -0.5*torch.ones(3)\n",
    "xover = -xunder\n",
    "\n",
    "diag_bounds = model.get_diag_bounds(xunder, xover)\n",
    "#print(diag_bounds)\n",
    "\n",
    "Du_lower, Du_upper = model.compute_Du_bounds(xunder, xover, elision_matrix=B)\n",
    "jac_lower, jac_upper = jac_bounds(xunder, xover)\n",
    "\n",
    "'''\n",
    "Want bound on P @ Df + Df^T P + P B Du + (P B Du)^T\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0926fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1000/20000] Loss: 445.1563 \n",
      "Epoch [2000/20000] Loss: 121.8278 \n",
      "Epoch [3000/20000] Loss: 43.8706 \n",
      "Epoch [4000/20000] Loss: 16.3270 \n",
      "Epoch [5000/20000] Loss: 4.5310 \n",
      "Epoch [6000/20000] Loss: 2.4599 \n",
      "Epoch [7000/20000] Loss: 0.0079 \n",
      "At epoch  7009  loss has hit 0, valid closed-loop contracting controller\n",
      "Metzler upper bound: tensor([[-4.1995e+02,  2.6099e+00,  1.4792e+01],\n",
      "        [ 2.6099e+00, -1.0168e+00,  5.5970e+00],\n",
      "        [ 1.4792e+01,  5.5970e+00, -1.3082e+03]], grad_fn=<SelectBackward0>)\n",
      "tensor([[-1.3084e+03, -4.1972e+02, -9.7579e-01],\n",
      "        [-1.3083e+03, -4.1976e+02, -9.4093e-01],\n",
      "        [-1.3064e+03, -4.1988e+02, -8.8992e-01],\n",
      "        ...,\n",
      "        [-1.6836e+03, -4.2197e+02, -8.2219e-01],\n",
      "        [-1.6775e+03, -4.2196e+02, -8.5921e-01],\n",
      "        [-1.6759e+03, -4.2193e+02, -8.7068e-01]],\n",
      "       grad_fn=<LinalgEighBackward0>)\n",
      "contraction metric tensor([[ 4.0063e+02, -4.9076e-03,  5.1186e-01],\n",
      "        [-4.9076e-03,  1.0001e+00,  1.9810e-02],\n",
      "        [ 5.1186e-01,  1.9810e-02,  7.4194e+00]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "eps = 1e-8\n",
    "learning_rate = 1e-2\n",
    "num_epochs = 20000\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "xunder = -10*torch.ones(3)\n",
    "eye = torch.eye(3).to(device=device)\n",
    "xover = 10*torch.ones(3)\n",
    "\n",
    "xunder, xover = partition_hyperrectangle(xunder, xover, 10**3)\n",
    "\n",
    "# --- Send model to device ---\n",
    "model = NN_IBP(hidden_dims=[32,32], trainable_NCM=True)\n",
    "model = model.to(device)\n",
    "\n",
    "# --- Loss and optimizer ---\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# --- Training loop ---\n",
    "for epoch in range(num_epochs):\n",
    "    # ---- Training ----\n",
    "    model.train()\n",
    "    # Check this function for bugs\n",
    "    NCM = model.constant_NCM()\n",
    "    Du_bounds = model.compute_Du_bounds(xunder, xover, elision_matrix=NCM @ B)\n",
    "\n",
    "    #print(Du_bounds[0].shape)\n",
    "    Df_lower, Df_upper = jac_bounds(xunder, xover)\n",
    "    Df_lower, Df_upper = model.left_multiply_by_constant_matrix(Df_lower, Df_upper, NCM)\n",
    "    #print(Df_bounds[0].shape)\n",
    "    # eigenbounds = max_eig_over_hyperrectangles(Df, xunder, xover, NCM)\n",
    "    B_Mzr = compute_metzler_upper_bound((Df_lower, Df_upper), Du_bounds)\n",
    "    # B_Mzr = compute_metzler_upper_bound_new(eigenbounds, Du_bounds)\n",
    "    # improve conditioning\n",
    "    B_Mzr = B_Mzr + eps * torch.eye(B_Mzr.shape[-1], device=B_Mzr.device).unsqueeze(0)\n",
    "    try:\n",
    "        eigs = torch.linalg.eigvalsh(B_Mzr)\n",
    "    except:\n",
    "        print('poor conditioning')\n",
    "        print(B_Mzr[0])\n",
    "        \n",
    "        break\n",
    "    max_eig = eigs.amax(dim=tuple(range(1, eigs.ndim)))\n",
    "    # max_eig = max_eig_metzler_shifted(B_Mzr)\n",
    "    # max_eig = B_Mzr.sum(dim=-1)\n",
    "    # print(max_eig)\n",
    "    loss = torch.sum(torch.relu(max_eig))\n",
    "    if torch.isnan(loss):\n",
    "        print('NaN detected in loss')\n",
    "        break\n",
    "\n",
    "    if loss <= 1e-7:\n",
    "        print('At epoch ', epoch+1, ' loss has hit 0, valid closed-loop contracting controller')\n",
    "        print('Metzler upper bound:', B_Mzr[0])\n",
    "        print(torch.linalg.eigvalsh(B_Mzr))\n",
    "        break\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None and torch.isnan(param.grad).any():\n",
    "            print(f\"NaN detected in gradients of {name}\")\n",
    "\n",
    "    if epoch % 1000 == 999:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "              f\"Loss: {loss:.4f} \")\n",
    "        \n",
    "\n",
    "print('contraction metric', model.constant_NCM())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "29dd6bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-9.3708e-05, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "xunder = -10.0*torch.ones(3)\n",
    "xover = 10.0*torch.ones(3)\n",
    "\n",
    "NCM = model.constant_NCM()\n",
    "\n",
    "xunder, xover = partition_hyperrectangle(xunder, xover, 10**3)\n",
    "\n",
    "Du_bounds = model.compute_Du_bounds(xunder, xover, elision_matrix=NCM @ B)\n",
    "Df_lower, Df_upper = jac_bounds(xunder, xover)\n",
    "Df_lower, Df_upper = model.left_multiply_by_constant_matrix(Df_lower, Df_upper, NCM)\n",
    "B_Mzr = compute_metzler_upper_bound((Df_lower, Df_upper), Du_bounds)\n",
    "\n",
    "\n",
    "B_Mzr = compute_metzler_upper_bound((Df_lower, Df_upper), Du_bounds)\n",
    "loss = (torch.max(torch.linalg.eigvalsh(B_Mzr)))\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c31a6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo:\n",
    "# 1. Make code modular / file for NN class + file for functions + file for training + file for dynamics\n",
    "# 1. Get 1 nice example working (pendulum maybe)\n",
    "# 2. Write code for general contraction metrics M(x) - perhaps another class for NCM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
